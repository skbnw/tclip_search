"""
S3ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ãƒ»è¡¨ç¤ºWebã‚¢ãƒ—ãƒª

Streamlitã‚’ä½¿ç”¨ã—ã¦ã€S3ãƒã‚±ãƒƒãƒˆå†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢ãƒ»è¡¨ç¤ºã—ã¾ã™ã€‚
- ç•ªçµ„IDï¼ˆdoc_idï¼‰ã§æ¤œç´¢
- ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
- ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
- ç”»åƒã®è¡¨ç¤º
"""

import streamlit as st
import boto3
import json
import sys
import os
from typing import Dict, List, Optional
from io import BytesIO
from datetime import date, time

# Windowsç’°å¢ƒã§ã®æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œ
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

# S3è¨­å®š
S3_BUCKET_NAME = "tclip-raw-data-2025"
S3_REGION = "ap-northeast-1"
S3_MASTER_PREFIX = "rag/master_text/"
S3_CHUNK_PREFIX = "rag/vector_chunks/"
S3_IMAGE_PREFIX = "rag/images/"

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="S3ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ã‚¢ãƒ—ãƒª",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚¿ã‚¤ãƒˆãƒ«
st.title("ğŸ” S3ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ãƒ»è¡¨ç¤ºã‚¢ãƒ—ãƒª")
st.markdown("---")

# AWSèªè¨¼æƒ…å ±ã®è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã€Streamlit Secretsã€ã¾ãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼‰
def get_aws_credentials():
    """AWSèªè¨¼æƒ…å ±ã‚’å–å¾—ï¼ˆå„ªå…ˆé †ä½: Secrets > ç’°å¢ƒå¤‰æ•° > ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼‰"""
    access_key = None
    secret_key = None
    region = S3_REGION
    
    # 1. Streamlit Secretsã‹ã‚‰å–å¾—ï¼ˆStreamlit Cloudã§ä½¿ç”¨ï¼‰
    try:
        if 'AWS_ACCESS_KEY_ID' in st.secrets:
            access_key = st.secrets['AWS_ACCESS_KEY_ID']
            secret_key = st.secrets['AWS_SECRET_ACCESS_KEY']
            region = st.secrets.get('AWS_DEFAULT_REGION', S3_REGION)
            return access_key, secret_key, region
    except (AttributeError, KeyError):
        pass
    
    # 2. ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
    access_key = os.getenv('AWS_ACCESS_KEY_ID')
    secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')
    if access_key and secret_key:
        return access_key, secret_key, os.getenv('AWS_DEFAULT_REGION', S3_REGION)
    
    # 3. ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šï¼‰
    return None, None, None

@st.cache_resource
def get_s3_client():
    """S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’è‡ªå‹•çš„ã«èª­ã¿è¾¼ã‚€ï¼‰"""
    try:
        # boto3ã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è‡ªå‹•çš„ã«èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€
        s3_client = boto3.client('s3', region_name=S3_REGION)
        return s3_client
    except Exception as e:
        st.error(f"S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        return None

# AWSèªè¨¼æƒ…å ±ã®å–å¾—
access_key, secret_key, region = get_aws_credentials()

# èªè¨¼æƒ…å ±ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šï¼ˆboto3ãŒè‡ªå‹•çš„ã«èª­ã¿è¾¼ã‚€ã‚ˆã†ã«ï¼‰
if access_key and secret_key:
    os.environ['AWS_ACCESS_KEY_ID'] = access_key
    os.environ['AWS_SECRET_ACCESS_KEY'] = secret_key
    os.environ['AWS_DEFAULT_REGION'] = region or S3_REGION

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    
    # AWSèªè¨¼æƒ…å ±ã®å…¥åŠ›ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    st.subheader("AWSèªè¨¼æƒ…å ±")
    
    # Secretsã‹ã‚‰å–å¾—ã§ããŸã‹ç¢ºèª
    has_secrets = False
    try:
        has_secrets = bool('AWS_ACCESS_KEY_ID' in st.secrets and 'AWS_SECRET_ACCESS_KEY' in st.secrets)
    except (AttributeError, KeyError):
        pass
    
    # ç’°å¢ƒå¤‰æ•°ã«èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    env_has_credentials = bool(os.getenv('AWS_ACCESS_KEY_ID') and os.getenv('AWS_SECRET_ACCESS_KEY'))
    
    # èªè¨¼æƒ…å ±ã®çŠ¶æ…‹ã‚’è¡¨ç¤º
    if has_secrets:
        st.success("âœ… Streamlit Secretsã‹ã‚‰èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    elif env_has_credentials:
        st.info("â„¹ï¸ ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    else:
        st.warning("âš ï¸ èªè¨¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š")
        st.markdown("1. Streamlit Cloud: Settings â†’ Secrets")
        st.markdown("2. ç’°å¢ƒå¤‰æ•°ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºæ™‚ï¼‰")
        st.markdown("3. ä¸‹è¨˜ã®å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆä¸€æ™‚çš„ï¼‰")
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ç”¨ï¼ˆSecretsã‚„ç’°å¢ƒå¤‰æ•°ãŒãªã„å ´åˆï¼‰
    if not has_secrets and not env_has_credentials:
        access_key_id = st.text_input(
            "Access Key ID", 
            value="",
            type="password",
            help="ä¸€æ™‚çš„ã«ä½¿ç”¨ã™ã‚‹å ´åˆã¯å…¥åŠ›ã—ã¦ãã ã•ã„"
        )
        secret_access_key = st.text_input(
            "Secret Access Key", 
            value="",
            type="password",
            help="ä¸€æ™‚çš„ã«ä½¿ç”¨ã™ã‚‹å ´åˆã¯å…¥åŠ›ã—ã¦ãã ã•ã„"
        )
        
        if access_key_id and secret_access_key:
            # å…¥åŠ›ã•ã‚ŒãŸèªè¨¼æƒ…å ±ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š
            os.environ['AWS_ACCESS_KEY_ID'] = access_key_id
            os.environ['AWS_SECRET_ACCESS_KEY'] = secret_access_key
            os.environ['AWS_DEFAULT_REGION'] = S3_REGION
            access_key = access_key_id
            secret_key = secret_access_key
            region = S3_REGION
    
    st.markdown("---")
    st.info("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: ç•ªçµ„IDï¼ˆdoc_idï¼‰ã§æ¤œç´¢ã§ãã¾ã™\n\nä¾‹: AkxAQAJ3gAM")

# S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å–å¾—ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰è‡ªå‹•çš„ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹ï¼‰
s3_client = get_s3_client()

if s3_client is None:
    st.error("S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚AWSèªè¨¼æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
st.header("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿æ¤œç´¢")

# æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®å–å¾—ï¼ˆåˆå›ã®ã¿èª­ã¿è¾¼ã¿ï¼‰
@st.cache_data(ttl=600)
def get_search_options(_s3_client) -> Dict[str, List[str]]:
    """æ¤œç´¢ç”¨ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆæ—¥ä»˜ã€æ™‚é–“ã€æ”¾é€å±€ï¼‰ã‚’å–å¾—"""
    try:
        all_masters = list_all_master_data(_s3_client)
        
        dates = set()
        times = set()
        channels = set()
        
        for master in all_masters:
            metadata = master.get('metadata', {})
            
            # æ—¥ä»˜
            if 'date' in metadata:
                date_str = str(metadata['date'])
                if date_str:
                    dates.add(date_str)
            
            # é–‹å§‹æ™‚é–“
            if 'start_time' in metadata:
                start_time = str(metadata['start_time'])
                if start_time:
                    times.add(start_time)
            
            # çµ‚äº†æ™‚é–“
            if 'end_time' in metadata:
                end_time = str(metadata['end_time'])
                if end_time:
                    times.add(end_time)
            
            # æ”¾é€å±€
            if 'channel' in metadata:
                channel = str(metadata['channel'])
                if channel:
                    channels.add(channel)
        
        return {
            'dates': sorted(list(dates)),
            'times': sorted(list(times)),
            'channels': sorted(list(channels))
        }
    except Exception as e:
        return {'dates': [], 'times': [], 'channels': []}

# æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ 
with st.form("search_form"):
    st.subheader("æ¤œç´¢æ¡ä»¶")
    
    # è¤‡æ•°åˆ—ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
    col1, col2 = st.columns(2)
    
    with col1:
        # ç•ªçµ„ID
        program_id = st.text_input(
            "ç•ªçµ„ID",
            placeholder="ä¾‹: AkxAQAJ3gAM",
            help="ç•ªçµ„IDï¼ˆdoc_idï¼‰ã‚’å…¥åŠ›"
        )
        
        # æ”¾é€å±€ï¼ˆé¸æŠå¼ï¼‰
        search_options = get_search_options(_s3_client=s3_client)
        channel = st.selectbox(
            "æ”¾é€å±€",
            options=["ã™ã¹ã¦"] + search_options['channels'],
            help="æ”¾é€å±€ã‚’é¸æŠã—ã¦ãã ã•ã„"
        )
    
    with col2:
        # æ—¥ä»˜ã¨æ™‚é–“ã‚’1ã¤ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã¾ã¨ã‚ã‚‹
        st.markdown("#### ğŸ“… æ—¥ä»˜ãƒ»æ™‚é–“ã§ãƒ•ã‚£ãƒ«ã‚¿")
        use_datetime_filter = st.checkbox(
            "æ—¥ä»˜ãƒ»æ™‚é–“ã§ãƒ•ã‚£ãƒ«ã‚¿ã‚’æœ‰åŠ¹ã«ã™ã‚‹", 
            key="use_datetime_filter", 
            help="ãƒã‚§ãƒƒã‚¯ã‚’å…¥ã‚Œã‚‹ã¨æ—¥ä»˜ã¨æ™‚é–“ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã™"
        )
        
        if use_datetime_filter:
            # æ—¥ä»˜ã¨æ™‚é–“ã‚’æ¨ªä¸¦ã³ã«é…ç½®
            datetime_col1, datetime_col2 = st.columns(2)
            
            with datetime_col1:
                st.markdown("**ğŸ“† æ—¥ä»˜**")
                selected_date = st.date_input(
                    "æ—¥ä»˜ã‚’é¸æŠ",
                    value=date.today(),
                    help="ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‹ã‚‰æ—¥ä»˜ã‚’é¸æŠã—ã¦ãã ã•ã„",
                    key="date_input",
                    label_visibility="collapsed"
                )
                date_str = selected_date.strftime("%Y%m%d") if selected_date else None
            
            with datetime_col2:
                st.markdown("**ğŸ• æ™‚é–“**")
                default_time = time(0, 0)  # 00:00ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                selected_time = st.time_input(
                    "æ™‚é–“ã‚’é¸æŠ",
                    value=default_time,
                    help="æ™‚é–“ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆæ™‚:åˆ†å½¢å¼ï¼‰",
                    key="time_input",
                    label_visibility="collapsed"
                )
                time_str = selected_time.strftime("%H%M") if selected_time else None
            
            # é¸æŠã•ã‚ŒãŸæ—¥ä»˜ã¨æ™‚é–“ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
            if selected_date and selected_time:
                preview_date = selected_date.strftime("%Yå¹´%mæœˆ%dæ—¥")
                preview_time = selected_time.strftime("%H:%M")
                st.info(f"ğŸ“Œ æ¤œç´¢æ¡ä»¶: {preview_date} {preview_time}")
        else:
            date_str = None
            time_str = None
            selected_date = None
            selected_time = None
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆå…¨æ–‡ã¨ãƒãƒ£ãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‚’å¯¾è±¡ï¼‰
    keyword = st.text_input(
        "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå…¨æ–‡ãƒ»ãƒãƒ£ãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ï¼‰",
        placeholder="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
        help="å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒãƒ£ãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¤œç´¢ã—ã¾ã™"
    )
    
    # æ¤œç´¢ãƒœã‚¿ãƒ³
    search_button = st.form_submit_button("ğŸ” æ¤œç´¢", use_container_width=True)

# ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•°
@st.cache_data(ttl=300)  # 5åˆ†é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def get_master_data(_s3_client, doc_id: str) -> Optional[Dict]:
    """ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    try:
        key = f"{S3_MASTER_PREFIX}{doc_id}.jsonl"
        response = _s3_client.get_object(Bucket=S3_BUCKET_NAME, Key=key)
        content = response['Body'].read().decode('utf-8')
        
        # JSON Lineså½¢å¼ãªã®ã§ã€æœ€åˆã®è¡Œã‚’èª­ã¿è¾¼ã‚€
        lines = content.strip().split('\n')
        if lines:
            return json.loads(lines[0])
        return None
    except _s3_client.exceptions.NoSuchKey:
        return None
    except Exception as e:
        st.error(f"ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

@st.cache_data(ttl=300)
def get_chunk_data(_s3_client, doc_id: str) -> List[Dict]:
    """ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    try:
        key = f"{S3_CHUNK_PREFIX}{doc_id}_segments.jsonl"
        response = _s3_client.get_object(Bucket=S3_BUCKET_NAME, Key=key)
        content = response['Body'].read().decode('utf-8')
        
        chunks = []
        for line in content.strip().split('\n'):
            if line:
                chunks.append(json.loads(line))
        return chunks
    except _s3_client.exceptions.NoSuchKey:
        return []
    except Exception as e:
        st.error(f"ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

@st.cache_data(ttl=300)
def list_images(_s3_client, doc_id: str) -> List[str]:
    """ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    try:
        prefix = f"{S3_IMAGE_PREFIX}{doc_id}/"
        response = _s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=prefix)
        
        images = []
        if 'Contents' in response:
            for obj in response['Contents']:
                if obj['Key'].endswith('.jpeg') or obj['Key'].endswith('.jpg'):
                    images.append(obj['Key'])
        return sorted(images)
    except Exception as e:
        st.error(f"ç”»åƒä¸€è¦§ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

@st.cache_data(ttl=600)  # 10åˆ†é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆã¯é‡ã„ãŸã‚ï¼‰
def list_all_master_data(_s3_client) -> List[Dict]:
    """å…¨ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆæ¤œç´¢ç”¨ï¼‰"""
    try:
        response = _s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=S3_MASTER_PREFIX)
        
        master_list = []
        if 'Contents' in response:
            for obj in response['Contents']:
                try:
                    # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
                    file_response = _s3_client.get_object(Bucket=S3_BUCKET_NAME, Key=obj['Key'])
                    content = file_response['Body'].read().decode('utf-8')
                    lines = content.strip().split('\n')
                    if lines:
                        master_data = json.loads(lines[0])
                        master_list.append(master_data)
                except Exception as e:
                    continue  # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—
        
        return master_list
    except Exception as e:
        st.error(f"å…¨ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def search_master_data_advanced(
    master_list: List[Dict], 
    program_id: str = "",
    date_str: str = "",
    time_str: str = "",
    channel: str = "",
    keyword: str = ""
) -> List[Dict]:
    """ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è©³ç´°æ¡ä»¶ã§æ¤œç´¢"""
    results = []
    
    for master in master_list:
        metadata = master.get('metadata', {})
        doc_id = master.get('doc_id', '')
        
        # å„æ¡ä»¶ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        match = True
        
        # ç•ªçµ„IDã§ãƒ•ã‚£ãƒ«ã‚¿
        if program_id and program_id.strip():
            if program_id.strip().lower() not in doc_id.lower():
                match = False
                continue
        
        # æ—¥ä»˜ã§ãƒ•ã‚£ãƒ«ã‚¿
        if date_str:
            master_date = str(metadata.get('date', ''))
            # æ—¥ä»˜å½¢å¼ã‚’å¤‰æ›ã—ã¦æ¯”è¼ƒï¼ˆYYYYMMDDå½¢å¼ï¼‰
            if date_str not in master_date:
                match = False
                continue
        
        # æ™‚é–“ã§ãƒ•ã‚£ãƒ«ã‚¿
        if time_str:
            start_time = str(metadata.get('start_time', ''))
            end_time = str(metadata.get('end_time', ''))
            # æ™‚é–“å½¢å¼ã‚’å¤‰æ›ã—ã¦æ¯”è¼ƒï¼ˆHHMMå½¢å¼ï¼‰
            # é–‹å§‹æ™‚é–“ã¾ãŸã¯çµ‚äº†æ™‚é–“ã«ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
            if time_str not in start_time and time_str not in end_time:
                match = False
                continue
        
        # æ”¾é€å±€ã§ãƒ•ã‚£ãƒ«ã‚¿
        if channel and channel != "ã™ã¹ã¦":
            master_channel = str(metadata.get('channel', ''))
            if channel not in master_channel:
                match = False
                continue
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå…¨æ–‡ã¨ãƒãƒ£ãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆï¼‰
        if keyword and keyword.strip():
            keyword_lower = keyword.strip().lower()
            keyword_match = False
            
            # å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã§æ¤œç´¢
            full_text = master.get('full_text', '').lower()
            if keyword_lower in full_text:
                keyword_match = True
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã§æ¤œç´¢ï¼ˆãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼‰
            # ãŸã ã—ã€å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ã™ã‚‹ã®ã¯é‡ã„ã®ã§ã€ã“ã“ã§ã¯ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒã‚§ãƒƒã‚¯
            # ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢ã¯åˆ¥é€”å®Ÿè£…ã™ã‚‹
            
            if not keyword_match:
                match = False
                continue
        
        if match:
            results.append(master)
    
    return results

def search_master_data_with_chunks(
    _s3_client,
    master_list: List[Dict], 
    program_id: str = "",
    date_str: str = "",
    time_str: str = "",
    channel: str = "",
    keyword: str = ""
) -> List[Dict]:
    """ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ£ãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€è©³ç´°æ¤œç´¢ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
    # ã¾ãšåŸºæœ¬æ¡ä»¶ã§ãƒ•ã‚£ãƒ«ã‚¿
    filtered_masters = search_master_data_advanced(
        master_list, program_id, date_str, time_str, channel, ""
    )
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    if keyword and keyword.strip():
        keyword_lower = keyword.strip().lower()
        results = []
        
        # ã¾ãšå…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆé«˜é€Ÿï¼‰
        full_text_matches = []
        chunk_candidates = []
        
        for master in filtered_masters:
            full_text = master.get('full_text', '').lower()
            if keyword_lower in full_text:
                full_text_matches.append(master)
            else:
                # å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã«ãƒãƒƒãƒã—ãªã„å ´åˆã®ã¿ã€ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢ã®å€™è£œã«
                chunk_candidates.append(master)
        
        results.extend(full_text_matches)
        
        # ãƒãƒ£ãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã§æ¤œç´¢ï¼ˆå…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã«ãƒãƒƒãƒã—ãªã‹ã£ãŸã‚‚ã®ã®ã¿ï¼‰
        # å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯ã€å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã§ã®ãƒãƒƒãƒã‚’å„ªå…ˆã—ã€ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢ã‚’æœ€å°åŒ–
        if chunk_candidates:
            for master in chunk_candidates:
                doc_id = master.get('doc_id', '')
                keyword_match = False
                
                try:
                    # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                    chunks = get_chunk_data(_s3_client=_s3_client, doc_id=doc_id)
                    
                    # ãƒãƒ£ãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã§æ¤œç´¢ï¼ˆæœ€åˆã«ãƒãƒƒãƒã—ãŸã‚‰å³åº§ã«çµ‚äº†ï¼‰
                    for chunk in chunks:
                        chunk_text = chunk.get('text', '').lower()
                        if keyword_lower in chunk_text:
                            keyword_match = True
                            break
                except:
                    pass  # ãƒãƒ£ãƒ³ã‚¯å–å¾—ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
                
                if keyword_match:
                    results.append(master)
        
        return results
    
    return filtered_masters

def get_image_url(s3_client, key: str, expires_in: int = 3600) -> str:
    """ç”»åƒã®ãƒ—ãƒªã‚µã‚¤ãƒ³ãƒ‰URLã‚’ç”Ÿæˆ"""
    try:
        url = s3_client.generate_presigned_url(
            'get_object',
            Params={'Bucket': S3_BUCKET_NAME, 'Key': key},
            ExpiresIn=expires_in
        )
        return url
    except Exception as e:
        st.error(f"ç”»åƒURLã®ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return ""

def display_master_data(master_data, chunks, images, doc_id):
    """ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º"""
    if master_data is None and not chunks and not images:
        st.info("ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # ã‚¿ãƒ–ã§è¡¨ç¤º
    tab1, tab2, tab3 = st.tabs(["ğŸ“„ ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿", "ğŸ“ ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿", "ğŸ–¼ï¸ ç”»åƒ"])
    
    # ã‚¿ãƒ–1: ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿
    with tab1:
        if master_data:
            st.subheader("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
            if 'metadata' in master_data:
                metadata = master_data['metadata']
                col1, col2 = st.columns(2)
                
                with col1:
                    st.json(metadata)
                
                with col2:
                    st.write("### ä¸»è¦æƒ…å ±")
                    if 'event_id' in metadata:
                        st.write(f"**Event ID**: `{metadata['event_id']}`")
                    if 'channel' in metadata:
                        st.write(f"**Channel**: {metadata['channel']}")
                    if 'date' in metadata:
                        st.write(f"**Date**: {metadata['date']}")
            
            st.markdown("---")
            st.subheader("ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ")
            
            if 'full_text' in master_data:
                full_text = master_data['full_text']
                st.text_area(
                    "å…¨æ–‡",
                    full_text,
                    height=300,
                    disabled=True,
                    help="ç•ªçµ„å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆ"
                )
                st.caption(f"æ–‡å­—æ•°: {len(full_text):,} æ–‡å­—")
            else:
                st.info("ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            
            # ç”»åƒURLãŒã‚ã‚‹å ´åˆ
            if 'image_urls' in master_data and master_data['image_urls']:
                st.markdown("---")
                st.subheader("é–¢é€£ç”»åƒ")
                st.write(f"ç”»åƒæ•°: {len(master_data['image_urls'])} æš")
        else:
            st.info("ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    # ã‚¿ãƒ–2: ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
    with tab2:
        if chunks:
            st.subheader(f"ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ ({len(chunks)} å€‹)")
            
            # ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚­ãƒ¼ã‚’ä»˜ä¸ï¼‰
            chunk_search = st.text_input(
                "ãƒãƒ£ãƒ³ã‚¯å†…ã‚’æ¤œç´¢", 
                placeholder="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›",
                key=f"chunk_search_{doc_id}"
            )
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_chunks = chunks
            if chunk_search:
                filtered_chunks = [
                    chunk for chunk in chunks
                    if chunk_search.lower() in chunk.get('text', '').lower()
                ]
                st.caption(f"æ¤œç´¢çµæœ: {len(filtered_chunks)} / {len(chunks)} å€‹ã®ãƒãƒ£ãƒ³ã‚¯")
            
            # ãƒãƒ£ãƒ³ã‚¯è¡¨ç¤º
            for i, chunk in enumerate(filtered_chunks):
                with st.expander(f"ãƒãƒ£ãƒ³ã‚¯ {i+1}: {chunk.get('chunk_id', 'N/A')}"):
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        st.write("**ãƒ†ã‚­ã‚¹ãƒˆ**:")
                        st.write(chunk.get('text', ''))
                    
                    with col2:
                        st.write("**ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿**:")
                        if 'metadata' in chunk:
                            metadata = chunk['metadata']
                            if 'start_time' in metadata:
                                st.write(f"é–‹å§‹æ™‚åˆ»: {metadata['start_time']}")
                            if 'end_time' in metadata:
                                st.write(f"çµ‚äº†æ™‚åˆ»: {metadata['end_time']}")
                            if 'original_file_path' in metadata:
                                st.caption(f"ãƒ‘ã‚¹: {metadata['original_file_path']}")
                    
                    st.caption(f"æ–‡å­—æ•°: {len(chunk.get('text', ''))} æ–‡å­—")
        else:
            st.info("ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    # ã‚¿ãƒ–3: ç”»åƒ
    with tab3:
        if images:
            st.subheader(f"ç”»åƒ ({len(images)} æš)")
            
            # ç”»åƒã‚’ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤º
            cols = st.columns(3)
            for idx, image_key in enumerate(images):
                col = cols[idx % 3]
                
                with col:
                    try:
                        image_url = get_image_url(s3_client, image_key)
                        if image_url:
                            st.image(image_url, use_container_width=True)
                            filename = image_key.split('/')[-1]
                            st.caption(filename)
                        else:
                            st.error("ç”»åƒã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
        else:
            st.info("ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    # çµ±è¨ˆæƒ…å ±
    st.markdown("---")
    st.subheader("ğŸ“Š çµ±è¨ˆæƒ…å ±")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿", "ã‚ã‚Š" if master_data else "ãªã—")
    with col2:
        st.metric("ãƒãƒ£ãƒ³ã‚¯æ•°", len(chunks))
    with col3:
        st.metric("ç”»åƒæ•°", len(images))
    with col4:
        if master_data and 'full_text' in master_data:
            st.metric("æ–‡å­—æ•°", f"{len(master_data['full_text']):,}")

# æ¤œç´¢å®Ÿè¡Œ
if search_button:
    # ç•ªçµ„IDã®ã¿ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ç›´æ¥å–å¾—
    if program_id and program_id.strip() and not date_str and not time_str and not channel and not keyword:
        with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."):
            doc_id = program_id.strip()
            master_data = get_master_data(_s3_client=s3_client, doc_id=doc_id)
            chunks = get_chunk_data(_s3_client=s3_client, doc_id=doc_id)
            images = list_images(_s3_client=s3_client, doc_id=doc_id)
        
        if master_data is None and not chunks and not images:
            st.error(f"âŒ ç•ªçµ„ID '{doc_id}' ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            st.info("ğŸ’¡ æ­£ã—ã„ç•ªçµ„IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        else:
            # ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
            st.success(f"âœ… ç•ªçµ„ID '{doc_id}' ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
            st.markdown("---")
            display_master_data(master_data, chunks, images, doc_id)
    else:
        # è¤‡æ•°æ¡ä»¶ã¾ãŸã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
        if not program_id and not date_str and not time_str and not channel and not keyword:
            st.warning("âš ï¸ æ¤œç´¢æ¡ä»¶ã‚’1ã¤ä»¥ä¸Šå…¥åŠ›ã—ã¦ãã ã•ã„")
        else:
            # å…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¤œç´¢
            with st.spinner("å…¨ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...ï¼ˆåˆå›ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰"):
                all_masters = list_all_master_data(_s3_client=s3_client)
            
            if not all_masters:
                st.error("âŒ ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            else:
                # æ¤œç´¢æ¡ä»¶ã®è¡¨ç¤º
                search_conditions = []
                if program_id:
                    search_conditions.append(f"ç•ªçµ„ID: {program_id}")
                if date_str:
                    search_conditions.append(f"æ—¥ä»˜: {date_str}")
                if time_str:
                    search_conditions.append(f"æ™‚é–“: {time_str}")
                if channel and channel != "ã™ã¹ã¦":
                    search_conditions.append(f"æ”¾é€å±€: {channel}")
                if keyword:
                    search_conditions.append(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}")
                
                with st.spinner(f"æ¤œç´¢ä¸­: {', '.join(search_conditions) if search_conditions else 'æ¡ä»¶ãªã—'}..."):
                    search_results = search_master_data_with_chunks(
                        _s3_client=s3_client,
                        master_list=all_masters,
                        program_id=program_id,
                        date_str=date_str if date_str else "",
                        time_str=time_str if time_str else "",
                        channel=channel if channel != "ã™ã¹ã¦" else "",
                        keyword=keyword
                    )
                
                if not search_results:
                    st.warning("âš ï¸ æ¤œç´¢æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                else:
                    st.success(f"âœ… {len(search_results)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                    st.markdown("---")
                    
                    # æ¤œç´¢çµæœã®è¡¨ç¤º
                    for idx, master in enumerate(search_results):
                        doc_id = master.get('doc_id', 'N/A')
                        metadata = master.get('metadata', {})
                        
                        # çµæœã®ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
                        result_header = f"çµæœ {idx+1}: {doc_id}"
                        if 'channel' in metadata:
                            result_header += f" ({metadata['channel']})"
                        if 'date' in metadata:
                            result_header += f" - {metadata['date']}"
                        
                        with st.expander(result_header, expanded=(idx == 0)):
                            # ã“ã®ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ã‚’è¡¨ç¤º
                            with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."):
                                full_master_data = get_master_data(_s3_client=s3_client, doc_id=doc_id)
                                chunks = get_chunk_data(_s3_client=s3_client, doc_id=doc_id)
                                images = list_images(_s3_client=s3_client, doc_id=doc_id)
                            
                            display_master_data(full_master_data, chunks, images, doc_id)

else:
    # åˆæœŸçŠ¶æ…‹ã®èª¬æ˜
    st.info("ğŸ‘ˆ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§AWSèªè¨¼æƒ…å ±ã‚’è¨­å®šã—ã€æ¤œç´¢æ¡ä»¶ã‚’å…¥åŠ›ã—ã¦æ¤œç´¢ã—ã¦ãã ã•ã„")
    
    st.markdown("---")
    st.subheader("ğŸ“– ä½¿ã„æ–¹")
    st.markdown("""
    1. **AWSèªè¨¼æƒ…å ±ã®è¨­å®š**
       - ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã€Œç’°å¢ƒå¤‰æ•°ã‚’ä½¿ç”¨ã€ã«ãƒã‚§ãƒƒã‚¯ï¼ˆç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ãŸå ´åˆï¼‰
       - ã¾ãŸã¯ã€Access Key IDã¨Secret Access Keyã‚’ç›´æ¥å…¥åŠ›
    
    2. **æ¤œç´¢æ¡ä»¶ã®å…¥åŠ›**
       - **ç•ªçµ„ID**: ç•ªçµ„IDï¼ˆdoc_idï¼‰ã‚’ç›´æ¥å…¥åŠ›
       - **æ—¥ä»˜**: ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‹ã‚‰æ—¥ä»˜ã‚’é¸æŠï¼ˆã€Œã™ã¹ã¦ã€ã‚’é¸æŠã™ã‚‹ã¨å…¨ä»¶ï¼‰
       - **æ™‚é–“**: ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‹ã‚‰æ™‚é–“ã‚’é¸æŠï¼ˆã€Œã™ã¹ã¦ã€ã‚’é¸æŠã™ã‚‹ã¨å…¨ä»¶ï¼‰
       - **æ”¾é€å±€**: ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‹ã‚‰æ”¾é€å±€ã‚’é¸æŠï¼ˆã€Œã™ã¹ã¦ã€ã‚’é¸æŠã™ã‚‹ã¨å…¨ä»¶ï¼‰
       - **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒãƒ£ãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¤œç´¢
    
    3. **æ¤œç´¢ã®å®Ÿè¡Œ**
       - ã€ŒğŸ” æ¤œç´¢ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
       - è¤‡æ•°ã®æ¡ä»¶ã‚’çµ„ã¿åˆã‚ã›ã¦æ¤œç´¢å¯èƒ½
    
    4. **ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º**
       - **ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚¿ãƒ–**: ç•ªçµ„ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆ
       - **ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚¿ãƒ–**: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå˜ä½ã®ãƒãƒ£ãƒ³ã‚¯ï¼ˆãƒãƒ£ãƒ³ã‚¯å†…æ¤œç´¢æ©Ÿèƒ½ã‚ã‚Šï¼‰
       - **ç”»åƒã‚¿ãƒ–**: screenshotsé…åˆ—ã«å«ã¾ã‚Œã‚‹ç”»åƒ
    """)

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.caption(f"ãƒã‚±ãƒƒãƒˆ: {S3_BUCKET_NAME} | ãƒªãƒ¼ã‚¸ãƒ§ãƒ³: {S3_REGION}")

