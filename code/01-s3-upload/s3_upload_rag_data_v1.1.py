"""
S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ - RAGãƒ‡ãƒ¼ã‚¿å‡¦ç†
Version: 1.1
Date: 2025/11/02

æ©Ÿèƒ½:
- ãƒ­ãƒ¼ã‚«ãƒ«NASä¸Šã®çµ±åˆJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
- ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã§ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°
- ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
- JSON Lineså½¢å¼ã§S3ã«ä¿å­˜
- program-integrationé…ä¸‹ã‚’å†å¸°çš„ã«æ¢ç´¢
- ãƒ•ã‚¡ã‚¤ãƒ«åã« "q1.00" ãŒå«ã¾ã‚Œã‚‹å®Œæˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å‡¦ç†ï¼ˆãƒãƒƒãƒå‡¦ç†å¯¾å¿œï¼‰
  ï¼ˆä¾‹: NHKG_TKY_20251015_0035-0125_AkxAQAELAAM_integrated_q1.00.jsonï¼‰

å¤‰æ›´å±¥æ­´:
v1.1: program-integrationé…ä¸‹å…¨ä½“ã‚’æ¢ç´¢ã€q1.00ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒãƒå‡¦ç†ã«å¯¾å¿œ
      - ãƒã‚±ãƒƒãƒˆåã‚’ã€Œtclip-raw-data-2025ã€ã«ä¿®æ­£
      - ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ã€Œap-northeast-1ã€ã«æ˜ç¤ºçš„ã«è¨­å®š

ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:
- boto3: S3æ“ä½œ
- jsonlines: JSON Lineså½¢å¼ã®å‡¦ç†
"""

import json
import jsonlines
import uuid
import boto3
import os
import re
import sys
from typing import Dict, List, Any

# Windowsç’°å¢ƒã§ã®æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œ
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

# --- è¨­å®š ---
# S3è¨­å®š
S3_BUCKET_NAME = "tclip-raw-data-2025"
S3_REGION = "ap-northeast-1"  # ã‚¢ã‚¸ã‚¢ãƒ‘ã‚·ãƒ•ã‚£ãƒƒã‚¯ (æ±äº¬)
S3_MASTER_PREFIX = "rag/master_text/"
S3_CHUNK_PREFIX = "rag/vector_chunks/"
S3_CLIENT = boto3.client('s3', region_name=S3_REGION)

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
BASE_NAS_PATH = r"\\NAS-TKY-2504\database\program-integration"

# --- ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥ (ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã€å¤‰æ›´ãªã—) ---
def segment_based_chunking(transcripts: List[Dict], doc_id: str) -> List[Dict[str, Any]]:
    """
    ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’åŸºæœ¬ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã€æ™‚é–“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä»˜ä¸ã™ã‚‹ã€‚
    """
    chunks = []
    
    # ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆé…åˆ—ã®å„è¦ç´ ã‚’ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦åˆ©ç”¨
    for i, segment in enumerate(transcripts):
        if 'content' not in segment or 'file_path' not in segment:
             continue 
        
        # 'content'ä»¥å¤–ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯ã€å…ƒã®çµ±åˆJSONã‹ã‚‰ã‚³ãƒ”ãƒ¼
        # ğŸ’¡ start_time_msã¨end_time_msã¯ã€çµ±åˆJSONã®'transcripts'è¦ç´ ã‹ã‚‰ç›´æ¥æŠ½å‡ºã•ã‚Œã‚‹ã¨ä»®å®šï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªã®å•é¡Œã«ã‚ˆã‚Šã€ä»Šå›ã¯'content'ãŒã‚ã‚‹ã‹ã®ã¿ãƒã‚§ãƒƒã‚¯ï¼‰
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®æ–‡å­—åˆ—ã‹ã‚‰é–‹å§‹ãƒ»çµ‚äº†æ™‚é–“ã®æ–‡å­—åˆ—ã‚’æŠ½å‡ºï¼ˆã‚ˆã‚Šæ­£ç¢ºãªæ™‚é–“æƒ…å ±ãŒã‚ã‚Œã°ãã¡ã‚‰ã‚’ä½¿ç”¨ã™ã¹ãï¼‰
        time_match = re.search(r'(\d{8}-\d{6})-\d+-', segment.get('file_name', segment.get('file_path', '')))
        
        chunk_id = f"{doc_id}-p{i:04d}" # ä¸€æ„ãªãƒãƒ£ãƒ³ã‚¯IDï¼ˆdoc_id + ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ™‚é–“æƒ…å ±ã‚„å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ ¼ç´
        metadata = {
            "source": "transcript",
            # JSONã«æ™‚é–“æƒ…å ±ãŒç„¡ã„å ´åˆã€ã“ã“ã§ã¯ä¸€æ—¦ç©ºã«ã™ã‚‹ã‹ã€ã‚ˆã‚Šå …ç‰¢ãªæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦
            "start_time": segment.get('start_time'), # çµ±åˆJSONã®æ§‹é€ ã«ä¾å­˜
            "end_time": segment.get('end_time'),     # çµ±åˆJSONã®æ§‹é€ ã«ä¾å­˜
            "original_file_path": segment['file_path'] 
        }

        chunks.append({
            "chunk_id": chunk_id,
            "doc_id": doc_id,
            "text": segment['content'], # ãƒãƒ£ãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆ
            "level": "segment",
            "metadata": metadata
        })
        
    return chunks

# --- S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ï¼ˆå¤‰æ›´ãªã—ï¼‰ ---
def upload_to_s3(data_list: List[Dict], key: str):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’JSON Lineså½¢å¼ã§S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    """
    data_str = ""
    for item in data_list:
        data_str += json.dumps(item, ensure_ascii=False) + "\n"
        
    # S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ“ä½œ
    S3_CLIENT.put_object(
        Bucket=S3_BUCKET_NAME,
        Key=key,
        Body=data_str.encode('utf-8'),
        ContentType='application/jsonl; charset=utf-8'
    )
    print(f"[OK] S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: s3://{S3_BUCKET_NAME}/{key}")


# --- ãƒ•ã‚¡ã‚¤ãƒ«æ¢ç´¢é–¢æ•° ---
def find_q100_json_files(root_path: str) -> List[str]:
    """
    program-integrationé…ä¸‹ã‚’å†å¸°çš„ã«æ¢ç´¢ã—ã€ãƒ•ã‚¡ã‚¤ãƒ«åã« "q1.00" ãŒå«ã¾ã‚Œã‚‹JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¿”ã™
    ï¼ˆä¾‹: NHKG_TKY_20251015_0035-0125_AkxAQAELAAM_integrated_q1.00.jsonï¼‰
    """
    json_files = []
    
    if not os.path.exists(root_path):
        print(f"[ERROR] ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {root_path}")
        return json_files
    
    print(f"[INFO] ãƒ•ã‚¡ã‚¤ãƒ«æ¢ç´¢ã‚’é–‹å§‹: {root_path}")
    
    # å†å¸°çš„ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
    for root, dirs, files in os.walk(root_path):
        for filename in files:
            # ãƒ•ã‚¡ã‚¤ãƒ«åã« "q1.00" ãŒå«ã¾ã‚Œã€.json ã§çµ‚ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
            if filename.lower().endswith('.json') and 'q1.00' in filename.lower():
                file_path = os.path.join(root, filename)
                json_files.append(file_path)
                print(f"[INFO] q1.00ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹: {file_path}")
    
    return json_files

# --- å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–¢æ•° ---
def process_single_file(file_path: str) -> bool:
    """
    å˜ä¸€ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    æˆåŠŸã—ãŸå ´åˆã¯Trueã€å¤±æ•—ã—ãŸå ´åˆã¯Falseã‚’è¿”ã™
    """
    try:
        print(f"\n[INFO] ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚’é–‹å§‹: {file_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        with open(file_path, 'r', encoding='utf-8') as f:
            integrated_data = json.load(f)
        
        # event_idã®å–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¿½åŠ ï¼‰
        if 'program_metadata' not in integrated_data or 'event_id' not in integrated_data['program_metadata']:
            print(f"[ERROR] program_metadata.event_id ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return False
        
        doc_id = integrated_data['program_metadata']['event_id']
        print(f"[INFO] doc_id: {doc_id}")
        
        # transcriptsã®å­˜åœ¨ç¢ºèª
        if 'transcripts' not in integrated_data:
            print(f"[ERROR] transcripts ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return False
        
        # 1. ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        master_data = {
            "doc_id": doc_id,
            "metadata": integrated_data['program_metadata'],
            "full_text": "".join([t['content'] for t in integrated_data['transcripts'] if 'content' in t])
        }
        
        # 2. ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        all_chunks = segment_based_chunking(integrated_data['transcripts'], doc_id)
        
        if not all_chunks:
            print(f"[WARNING] ãƒãƒ£ãƒ³ã‚¯ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ: {file_path}")
            return False
        
        print(f"[INFO] {len(all_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆ")
        
        # --- S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ ---
        
        # A. ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ (PostgreSQLã®å…¥åŠ›ç”¨)
        master_key = f"{S3_MASTER_PREFIX}{doc_id}.jsonl"
        upload_to_s3([master_data], master_key)
        
        # B. ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ (Weaviate/OpenSearchã®å…¥åŠ›ç”¨)
        chunk_key = f"{S3_CHUNK_PREFIX}{doc_id}_segments.jsonl"
        upload_to_s3(all_chunks, chunk_key)
        
        print(f"[OK] ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {file_path}")
        return True
        
    except json.JSONDecodeError as e:
        print(f"[ERROR] JSONè§£æã‚¨ãƒ©ãƒ¼: {file_path} - {str(e)}")
        return False
    except Exception as e:
        print(f"[ERROR] å‡¦ç†ã‚¨ãƒ©ãƒ¼: {file_path} - {str(e)}")
        import traceback
        traceback.print_exc()
        return False

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† (ãƒãƒƒãƒå‡¦ç†å¯¾å¿œ) ---
def process_and_upload_local_rag_data():
    """
    program-integrationé…ä¸‹ã®q1.00ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢ã—ã€ãƒãƒƒãƒå‡¦ç†ã§S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    """
    # q1.00ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
    json_files = find_q100_json_files(BASE_NAS_PATH)
    
    if not json_files:
        print(f"[WARNING] q1.00ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    print(f"\n[INFO] åˆè¨ˆ {len(json_files)} å€‹ã®q1.00ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
    print("=" * 80)
    
    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    success_count = 0
    error_count = 0
    
    for i, file_path in enumerate(json_files, 1):
        print(f"\n[{i}/{len(json_files)}] å‡¦ç†ä¸­...")
        if process_single_file(file_path):
            success_count += 1
        else:
            error_count += 1
    
    # å‡¦ç†çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 80)
    print(f"[SUMMARY] å‡¦ç†å®Œäº†")
    print(f"  æˆåŠŸ: {success_count} ãƒ•ã‚¡ã‚¤ãƒ«")
    print(f"  å¤±æ•—: {error_count} ãƒ•ã‚¡ã‚¤ãƒ«")
    print(f"  åˆè¨ˆ: {len(json_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
    
# --- å®Ÿè¡Œä¾‹ ---
# å®Ÿè¡Œã™ã‚‹å‰ã«ã€BASE_NAS_PATHãŒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµŒç”±ã§Pythonã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
# process_and_upload_local_rag_data(CHANNEL_CODE, TARGET_EVENT_ID)

if __name__ == "__main__":
    # ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œï¼ˆq1.00ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒãƒå‡¦ç†ï¼‰
    process_and_upload_local_rag_data()

