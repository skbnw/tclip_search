"""
S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ - RAGãƒ‡ãƒ¼ã‚¿å‡¦ç†
Version: 1.3
Date: 2025/11/02

æ©Ÿèƒ½:
- ãƒ­ãƒ¼ã‚«ãƒ«NASä¸Šã®çµ±åˆJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
- ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã§ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°
- ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
- JSON Lineså½¢å¼ã§S3ã«ä¿å­˜
- program-integrationé…ä¸‹ã‚’å†å¸°çš„ã«æ¢ç´¢
- ãƒ•ã‚¡ã‚¤ãƒ«åã« "q1.00" ãŒå«ã¾ã‚Œã‚‹å®Œæˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å‡¦ç†ï¼ˆãƒãƒƒãƒå‡¦ç†å¯¾å¿œï¼‰
- ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆscreenshotsï¼‰ã®S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾å¿œ
- ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ï¼ˆæ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰
  ï¼ˆä¾‹: NHKG_TKY_20251015_0035-0125_AkxAQAELAAM_integrated_q1.00.jsonï¼‰

å¤‰æ›´å±¥æ­´:
v1.3: ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ã‚’è¿½åŠ 
      - S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å­˜åœ¨ç¢ºèªã¨æœ€çµ‚æ›´æ–°æ—¥æ™‚ã®æ¯”è¼ƒ
      - NASãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ—¥æ™‚ã¨S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æ›´æ–°æ—¥æ™‚ã‚’æ¯”è¼ƒ
      - æ›´æ–°ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿å†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆé‡è¤‡å›é¿ï¼‰
      - ã‚¹ã‚­ãƒƒãƒ—æ©Ÿèƒ½è¿½åŠ 
v1.2: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆscreenshotsï¼‰ã®S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ã‚’è¿½åŠ 
      - JSONå†…ã®Linuxãƒ‘ã‚¹ã‚’Windows NASãƒ‘ã‚¹ã«å¤‰æ›
      - screenshotsé…åˆ—ã‹ã‚‰ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã—ã¦S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
      - ç”»åƒURLã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«å«ã‚ã‚‹
v1.1: program-integrationé…ä¸‹å…¨ä½“ã‚’æ¢ç´¢ã€q1.00ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒãƒå‡¦ç†ã«å¯¾å¿œ
      - ãƒã‚±ãƒƒãƒˆåã‚’ã€Œtclip-raw-data-2025ã€ã«ä¿®æ­£
      - ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ã€Œap-northeast-1ã€ã«æ˜ç¤ºçš„ã«è¨­å®š

ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:
- boto3: S3æ“ä½œ
- jsonlines: JSON Lineså½¢å¼ã®å‡¦ç†
"""

import json
import jsonlines
import uuid
import boto3
import os
import re
import sys
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from datetime import datetime
from botocore.exceptions import ClientError

# Windowsç’°å¢ƒã§ã®æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œ
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

# --- è¨­å®š ---
# S3è¨­å®š
S3_BUCKET_NAME = "tclip-raw-data-2025"
S3_REGION = "ap-northeast-1"  # ã‚¢ã‚¸ã‚¢ãƒ‘ã‚·ãƒ•ã‚£ãƒƒã‚¯ (æ±äº¬)
S3_MASTER_PREFIX = "rag/master_text/"
S3_CHUNK_PREFIX = "rag/vector_chunks/"
S3_IMAGE_PREFIX = "rag/images/"  # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
S3_CLIENT = boto3.client('s3', region_name=S3_REGION)

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
BASE_NAS_PATH = r"\\NAS-TKY-2504\database\program-integration"
BASE_PROCESSED_NAS_PATH = r"\\NAS-TKY-2504\processed"

# --- ãƒ‘ã‚¹å¤‰æ›é–¢æ•° ---
def convert_linux_path_to_windows_nas(linux_path: str, channel_code: str = None, date_str: str = None) -> Optional[str]:
    r"""
    JSONå†…ã®Linuxãƒ‘ã‚¹å½¢å¼ã‚’Windows NASãƒ‘ã‚¹ã«å¤‰æ›ã™ã‚‹
    ä¾‹: /run/user/1000/gvfs/smb-share:server=nas-tky-2504.local,share=processed/NHKG-TKY/20251015AM/screenshot/xxx.jpeg
    -> \\NAS-TKY-2504\processed\NHKG-TKY\20251015AM\screenshot\xxx.jpeg
    
    ã¾ãŸã¯ screenshotsãƒ•ã‚©ãƒ«ãƒ€ã‚’è©¦è¡Œ
    """
    # Linuxãƒ‘ã‚¹ã‹ã‚‰ãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ—¥ä»˜ã€ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡º
    # ãƒ‘ã‚¿ãƒ¼ãƒ³: /run/user/.../share=processed/{CHANNEL}/{DATE}/screenshot(s)/{FILENAME}
    pattern = r'/share=processed/([^/]+)/([^/]+)/(?:screenshot|screenshots)/([^/]+\.jpeg)'
    match = re.search(pattern, linux_path)
    
    if not match:
        # ç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã‹ã‚‰æ§‹æˆã‚’è©¦è¡Œ
        filename = os.path.basename(linux_path)
        if channel_code and date_str:
            # ãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ—¥ä»˜ãŒæ—¢ã«åˆ†ã‹ã£ã¦ã„ã‚‹å ´åˆ
            for folder_name in ['screenshot', 'screenshots']:
                windows_path = os.path.join(BASE_PROCESSED_NAS_PATH, channel_code, date_str, folder_name, filename)
                if os.path.exists(windows_path):
                    return windows_path
        return None
    
    channel = match.group(1)
    date = match.group(2)
    filename = match.group(3)
    
    # screenshot ã¨ screenshots ã®ä¸¡æ–¹ã‚’è©¦è¡Œ
    for folder_name in ['screenshot', 'screenshots']:
        windows_path = os.path.join(BASE_PROCESSED_NAS_PATH, channel, date, folder_name, filename)
        if os.path.exists(windows_path):
            return windows_path
    
    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€åˆã®å€™è£œã‚’è¿”ã™ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ãŒã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¯å‘¼ã³å‡ºã—å´ã§ï¼‰
    return os.path.join(BASE_PROCESSED_NAS_PATH, channel, date, 'screenshot', filename)

# --- ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–¢æ•° ---
def upload_image_to_s3(image_path: str, doc_id: str, image_filename: str) -> Optional[str]:
    """
    ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    æˆ»ã‚Šå€¤: S3ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼ï¼ˆæˆåŠŸæ™‚ï¼‰ã€Noneï¼ˆå¤±æ•—æ™‚ï¼‰
    """
    try:
        if not os.path.exists(image_path):
            print(f"[WARNING] ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_path}")
            return None
        
        # S3ã®ã‚­ãƒ¼ã‚’ç”Ÿæˆ: rag/images/{doc_id}/{filename}
        # ãƒ•ã‚¡ã‚¤ãƒ«åã®è¡çªã‚’é¿ã‘ã‚‹ãŸã‚ã€doc_idã§ãƒ•ã‚©ãƒ«ãƒ€åˆ†ã‘
        s3_key = f"{S3_IMAGE_PREFIX}{doc_id}/{image_filename}"
        
        # Content-Typeã‚’è¨­å®š
        content_type = 'image/jpeg'
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        with open(image_path, 'rb') as f:
            S3_CLIENT.put_object(
                Bucket=S3_BUCKET_NAME,
                Key=s3_key,
                Body=f.read(),
                ContentType=content_type
            )
        
        s3_url = f"s3://{S3_BUCKET_NAME}/{s3_key}"
        return s3_url
        
    except Exception as e:
        print(f"[ERROR] ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {image_path} - {str(e)}")
        return None

# --- ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥ (ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã€å¤‰æ›´ãªã—) ---
def segment_based_chunking(transcripts: List[Dict], doc_id: str) -> List[Dict[str, Any]]:
    """
    ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’åŸºæœ¬ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã€æ™‚é–“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä»˜ä¸ã™ã‚‹ã€‚
    """
    chunks = []
    
    # ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆé…åˆ—ã®å„è¦ç´ ã‚’ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦åˆ©ç”¨
    for i, segment in enumerate(transcripts):
        if 'content' not in segment or 'file_path' not in segment:
             continue 
        
        # 'content'ä»¥å¤–ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯ã€å…ƒã®çµ±åˆJSONã‹ã‚‰ã‚³ãƒ”ãƒ¼
        # ğŸ’¡ start_time_msã¨end_time_msã¯ã€çµ±åˆJSONã®'transcripts'è¦ç´ ã‹ã‚‰ç›´æ¥æŠ½å‡ºã•ã‚Œã‚‹ã¨ä»®å®šï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªã®å•é¡Œã«ã‚ˆã‚Šã€ä»Šå›ã¯'content'ãŒã‚ã‚‹ã‹ã®ã¿ãƒã‚§ãƒƒã‚¯ï¼‰
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®æ–‡å­—åˆ—ã‹ã‚‰é–‹å§‹ãƒ»çµ‚äº†æ™‚é–“ã®æ–‡å­—åˆ—ã‚’æŠ½å‡ºï¼ˆã‚ˆã‚Šæ­£ç¢ºãªæ™‚é–“æƒ…å ±ãŒã‚ã‚Œã°ãã¡ã‚‰ã‚’ä½¿ç”¨ã™ã¹ãï¼‰
        time_match = re.search(r'(\d{8}-\d{6})-\d+-', segment.get('file_name', segment.get('file_path', '')))
        
        chunk_id = f"{doc_id}-p{i:04d}" # ä¸€æ„ãªãƒãƒ£ãƒ³ã‚¯IDï¼ˆdoc_id + ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ™‚é–“æƒ…å ±ã‚„å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ ¼ç´
        metadata = {
            "source": "transcript",
            # JSONã«æ™‚é–“æƒ…å ±ãŒç„¡ã„å ´åˆã€ã“ã“ã§ã¯ä¸€æ—¦ç©ºã«ã™ã‚‹ã‹ã€ã‚ˆã‚Šå …ç‰¢ãªæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦
            "start_time": segment.get('start_time'), # çµ±åˆJSONã®æ§‹é€ ã«ä¾å­˜
            "end_time": segment.get('end_time'),     # çµ±åˆJSONã®æ§‹é€ ã«ä¾å­˜
            "original_file_path": segment['file_path'] 
        }

        chunks.append({
            "chunk_id": chunk_id,
            "doc_id": doc_id,
            "text": segment['content'], # ãƒãƒ£ãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆ
            "level": "segment",
            "metadata": metadata
        })
        
    return chunks

# --- S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå­˜åœ¨ç¢ºèªã¨æ›´æ–°æ—¥æ™‚å–å¾— ---
def get_s3_object_metadata(key: str) -> Optional[Dict]:
    """
    S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
    æˆ»ã‚Šå€¤: {'exists': True, 'LastModified': datetime} ã¾ãŸã¯ None
    """
    try:
        response = S3_CLIENT.head_object(Bucket=S3_BUCKET_NAME, Key=key)
        return {
            'exists': True,
            'LastModified': response['LastModified'],
            'ETag': response.get('ETag', ''),
            'Size': response.get('ContentLength', 0)
        }
    except ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == '404':
            return {'exists': False}
        else:
            print(f"[WARNING] S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç¢ºèªã‚¨ãƒ©ãƒ¼: {key} - {error_code}")
            return None

# --- ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ãƒã‚§ãƒƒã‚¯ ---
def should_upload_file(file_path: str, s3_key: str) -> Tuple[bool, str]:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã¹ãã‹ãƒã‚§ãƒƒã‚¯
    æˆ»ã‚Šå€¤: (ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã¹ãã‹, ç†ç”±)
    """
    try:
        # NASãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ—¥æ™‚ã‚’å–å¾—
        file_mtime = os.path.getmtime(file_path)
        file_datetime = datetime.fromtimestamp(file_mtime)
        
        # S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        s3_metadata = get_s3_object_metadata(s3_key)
        
        if not s3_metadata or not s3_metadata.get('exists'):
            return (True, "S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå­˜åœ¨ã—ãªã„")
        
        # S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æœ€çµ‚æ›´æ–°æ—¥æ™‚ï¼ˆUTCï¼‰
        s3_last_modified = s3_metadata['LastModified']
        
        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’è€ƒæ…®ã—ã¦æ¯”è¼ƒ
        # S3ã¯UTCã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã¯ãƒ­ãƒ¼ã‚«ãƒ«æ™‚åˆ»ãªã®ã§ã€timezone-naiveã§æ¯”è¼ƒ
        # ä¸¡æ–¹ã‚’naive datetimeã«å¤‰æ›ã—ã¦æ¯”è¼ƒ
        s3_datetime_naive = s3_last_modified.replace(tzinfo=None) if s3_last_modified.tzinfo else s3_last_modified
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ—¥æ™‚ãŒS3ã‚ˆã‚Šæ–°ã—ã„å ´åˆã®ã¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        # 5ç§’ã®ãƒãƒ¼ã‚¸ãƒ³ã‚’è¨­å®šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³èª¤å·®ã‚’è€ƒæ…®ï¼‰
        time_diff = (file_datetime - s3_datetime_naive).total_seconds()
        if time_diff > 5:
            return (True, f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ›´æ–°ã•ã‚Œã¦ã„ã¾ã™ (NAS: {file_datetime}, S3: {s3_datetime_naive}, å·®åˆ†: {time_diff:.1f}ç§’)")
        else:
            return (False, f"ãƒ•ã‚¡ã‚¤ãƒ«ã¯æœ€æ–°ã§ã™ (ã‚¹ã‚­ãƒƒãƒ—)")
    
    except Exception as e:
        print(f"[WARNING] æ›´æ–°ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {file_path} - {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨ã®ãŸã‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        return (True, f"ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šæ›´æ–°ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—: {str(e)}")

# --- S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç† ---
def upload_to_s3(data_list: List[Dict], key: str, skip_if_exists: bool = False):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’JSON Lineså½¢å¼ã§S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    skip_if_exists: Trueã®å ´åˆã€æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ›´æ–°ãƒã‚§ãƒƒã‚¯ã¯å‘¼ã³å‡ºã—å´ã§å®Ÿæ–½ï¼‰
    """
    data_str = ""
    for item in data_list:
        data_str += json.dumps(item, ensure_ascii=False) + "\n"
        
    # S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ“ä½œ
    S3_CLIENT.put_object(
        Bucket=S3_BUCKET_NAME,
        Key=key,
        Body=data_str.encode('utf-8'),
        ContentType='application/jsonl; charset=utf-8'
    )
    print(f"[OK] S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: s3://{S3_BUCKET_NAME}/{key}")


# --- ãƒ•ã‚¡ã‚¤ãƒ«æ¢ç´¢é–¢æ•° ---
def find_q100_json_files(root_path: str) -> List[str]:
    """
    program-integrationé…ä¸‹ã‚’å†å¸°çš„ã«æ¢ç´¢ã—ã€ãƒ•ã‚¡ã‚¤ãƒ«åã« "q1.00" ãŒå«ã¾ã‚Œã‚‹JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¿”ã™
    ï¼ˆä¾‹: NHKG_TKY_20251015_0035-0125_AkxAQAELAAM_integrated_q1.00.jsonï¼‰
    """
    json_files = []
    
    if not os.path.exists(root_path):
        print(f"[ERROR] ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {root_path}")
        return json_files
    
    print(f"[INFO] ãƒ•ã‚¡ã‚¤ãƒ«æ¢ç´¢ã‚’é–‹å§‹: {root_path}")
    
    # å†å¸°çš„ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
    for root, dirs, files in os.walk(root_path):
        for filename in files:
            # ãƒ•ã‚¡ã‚¤ãƒ«åã« "q1.00" ãŒå«ã¾ã‚Œã€.json ã§çµ‚ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
            if filename.lower().endswith('.json') and 'q1.00' in filename.lower():
                file_path = os.path.join(root, filename)
                json_files.append(file_path)
                print(f"[INFO] q1.00ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹: {file_path}")
    
    return json_files

# --- ç”»åƒå‡¦ç†é–¢æ•° ---
def process_and_upload_images(integrated_data: Dict, doc_id: str) -> List[str]:
    """
    JSONå†…ã®screenshotsé…åˆ—ã‹ã‚‰ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    æˆ»ã‚Šå€¤: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã®S3 URLãƒªã‚¹ãƒˆ
    """
    uploaded_image_urls = []
    
    if 'screenshots' not in integrated_data or not integrated_data['screenshots']:
        print(f"[INFO] ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆdoc_id: {doc_id}ï¼‰")
        return uploaded_image_urls
    
    # ãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ—¥ä»˜ã‚’å–å¾—ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚„ãƒ‘ã‚¹ã‹ã‚‰æŠ½å‡ºã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯Noneã®ã¾ã¾ï¼ˆãƒ‘ã‚¹å¤‰æ›é–¢æ•°å†…ã§å‡¦ç†ï¼‰
    channel_code = None
    date_str = None
    
    print(f"[INFO] {len(integrated_data['screenshots'])}å€‹ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­...")
    
    for screenshot in integrated_data['screenshots']:
        linux_path = screenshot.get('file_path', '')
        filename = screenshot.get('file_name', '')
        
        if not linux_path or not filename:
            continue
        
        # ãƒ‘ã‚¹å¤‰æ›
        windows_path = convert_linux_path_to_windows_nas(linux_path, channel_code, date_str)
        
        if not windows_path:
            # ãƒ‘ã‚¹å¤‰æ›ã«å¤±æ•—ã—ãŸå ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç›´æ¥æ§‹ç¯‰ã‚’è©¦è¡Œ
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã¨ãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            # ä¾‹: NHKG-TKY-20251015-003534-xxx.jpeg
            filename_match = re.search(r'([A-Z]+-[A-Z]+)-(\d{8})', filename)
            if filename_match:
                channel = filename_match.group(1)
                date = filename_match.group(2)
                # æ—¥ä»˜å½¢å¼ã‚’å¤‰æ›: 20251015 -> 20251015AM ã¾ãŸã¯ä»–ã®å½¢å¼
                # ã“ã“ã§ã¯ä¸€æ—¦ãã®ã¾ã¾ä½¿ç”¨
                for folder_name in ['screenshot', 'screenshots']:
                    test_path = os.path.join(BASE_PROCESSED_NAS_PATH, channel, date, folder_name, filename)
                    if os.path.exists(test_path):
                        windows_path = test_path
                        break
                    # AM/PMãªã©ã®ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è©¦è¡Œ
                    for suffix in ['AM', 'PM']:
                        test_path = os.path.join(BASE_PROCESSED_NAS_PATH, channel, f"{date}{suffix}", folder_name, filename)
                        if os.path.exists(test_path):
                            windows_path = test_path
                            break
        
        if windows_path and os.path.exists(windows_path):
            s3_url = upload_image_to_s3(windows_path, doc_id, filename)
            if s3_url:
                uploaded_image_urls.append(s3_url)
                print(f"[OK] ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {s3_url}")
            else:
                print(f"[WARNING] ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {windows_path}")
        else:
            print(f"[WARNING] ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {linux_path} (å¤‰æ›å¾Œ: {windows_path})")
    
    return uploaded_image_urls

# --- å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–¢æ•° ---
def process_single_file(file_path: str) -> bool:
    """
    å˜ä¸€ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    æˆåŠŸã—ãŸå ´åˆã¯Trueã€å¤±æ•—ã—ãŸå ´åˆã¯Falseã‚’è¿”ã™
    """
    try:
        print(f"\n[INFO] ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚’é–‹å§‹: {file_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        with open(file_path, 'r', encoding='utf-8') as f:
            integrated_data = json.load(f)
        
        # event_idã®å–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¿½åŠ ï¼‰
        if 'program_metadata' not in integrated_data or 'event_id' not in integrated_data['program_metadata']:
            print(f"[ERROR] program_metadata.event_id ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return False
        
        doc_id = integrated_data['program_metadata']['event_id']
        print(f"[INFO] doc_id: {doc_id}")
        
        # transcriptsã®å­˜åœ¨ç¢ºèª
        if 'transcripts' not in integrated_data:
            print(f"[ERROR] transcripts ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return False
        
        # 1. ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        master_data = {
            "doc_id": doc_id,
            "metadata": integrated_data['program_metadata'],
            "full_text": "".join([t['content'] for t in integrated_data['transcripts'] if 'content' in t])
        }
        
        # 2. ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        all_chunks = segment_based_chunking(integrated_data['transcripts'], doc_id)
        
        if not all_chunks:
            print(f"[WARNING] ãƒãƒ£ãƒ³ã‚¯ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ: {file_path}")
            return False
        
        print(f"[INFO] {len(all_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆ")
        
        # --- æ›´æ–°ãƒã‚§ãƒƒã‚¯ ---
        master_key = f"{S3_MASTER_PREFIX}{doc_id}.jsonl"
        chunk_key = f"{S3_CHUNK_PREFIX}{doc_id}_segments.jsonl"
        
        # ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ãƒã‚§ãƒƒã‚¯
        should_upload_master, reason_master = should_upload_file(file_path, master_key)
        should_upload_chunk, reason_chunk = should_upload_file(file_path, chunk_key)
        
        # ã©ã¡ã‚‰ã‹ãŒæ›´æ–°ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not should_upload_master and not should_upload_chunk:
            print(f"[SKIP] ãƒ•ã‚¡ã‚¤ãƒ«ã¯æœ€æ–°ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {file_path}")
            print(f"  ç†ç”±: {reason_master}")
            return True
        
        if should_upload_master:
            print(f"[INFO] ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°: {reason_master}")
        if should_upload_chunk:
            print(f"[INFO] ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°: {reason_chunk}")
        
        # 3. ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        uploaded_image_urls = process_and_upload_images(integrated_data, doc_id)
        
        # ç”»åƒURLã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
        if uploaded_image_urls:
            master_data['image_urls'] = uploaded_image_urls
            print(f"[INFO] {len(uploaded_image_urls)}å€‹ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
        
        # --- S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ ---
        
        # A. ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ (PostgreSQLã®å…¥åŠ›ç”¨)
        if should_upload_master:
            upload_to_s3([master_data], master_key)
        else:
            print(f"[SKIP] ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {master_key}")
        
        # B. ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ (Weaviate/OpenSearchã®å…¥åŠ›ç”¨)
        if should_upload_chunk:
            upload_to_s3(all_chunks, chunk_key)
        else:
            print(f"[SKIP] ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {chunk_key}")
        
        print(f"[OK] ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {file_path}")
        return True
        
    except json.JSONDecodeError as e:
        print(f"[ERROR] JSONè§£æã‚¨ãƒ©ãƒ¼: {file_path} - {str(e)}")
        return False
    except Exception as e:
        print(f"[ERROR] å‡¦ç†ã‚¨ãƒ©ãƒ¼: {file_path} - {str(e)}")
        import traceback
        traceback.print_exc()
        return False

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† (ãƒãƒƒãƒå‡¦ç†å¯¾å¿œ) ---
def process_and_upload_local_rag_data():
    """
    program-integrationé…ä¸‹ã®q1.00ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢ã—ã€ãƒãƒƒãƒå‡¦ç†ã§S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    """
    # q1.00ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
    json_files = find_q100_json_files(BASE_NAS_PATH)
    
    if not json_files:
        print(f"[WARNING] q1.00ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    print(f"\n[INFO] åˆè¨ˆ {len(json_files)} å€‹ã®q1.00ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
    print("=" * 80)
    
    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    success_count = 0
    error_count = 0
    
    for i, file_path in enumerate(json_files, 1):
        print(f"\n[{i}/{len(json_files)}] å‡¦ç†ä¸­...")
        if process_single_file(file_path):
            success_count += 1
        else:
            error_count += 1
    
    # å‡¦ç†çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 80)
    print(f"[SUMMARY] å‡¦ç†å®Œäº†")
    print(f"  æˆåŠŸ: {success_count} ãƒ•ã‚¡ã‚¤ãƒ«")
    print(f"  å¤±æ•—: {error_count} ãƒ•ã‚¡ã‚¤ãƒ«")
    print(f"  åˆè¨ˆ: {len(json_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
    
# --- å®Ÿè¡Œä¾‹ ---
# å®Ÿè¡Œã™ã‚‹å‰ã«ã€BASE_NAS_PATHãŒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµŒç”±ã§Pythonã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

if __name__ == "__main__":
    # ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œï¼ˆq1.00ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒãƒå‡¦ç† + ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰
    process_and_upload_local_rag_data()

