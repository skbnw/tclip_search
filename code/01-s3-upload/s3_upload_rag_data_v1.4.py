"""
S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ - RAGãƒ‡ãƒ¼ã‚¿å‡¦ç†
Version: 1.4
Date: 2025/11/02

æ©Ÿèƒ½:
- ãƒ­ãƒ¼ã‚«ãƒ«NASä¸Šã®çµ±åˆJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
- ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã§ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°
- ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
- JSON Lineså½¢å¼ã§S3ã«ä¿å­˜
- program-integrationé…ä¸‹ã‚’å†å¸°çš„ã«æ¢ç´¢
- ã™ã¹ã¦ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ï¼ˆq1.00ä»¥å¤–ã‚‚å«ã‚€ï¼‰
- é¡ä¼¼ãƒ•ã‚¡ã‚¤ãƒ«åãŒã‚ã‚‹å ´åˆã€q1.00ã«è¿‘ã„æ–¹ã‚’å„ªå…ˆ
- ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆscreenshotsï¼‰ã®S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾å¿œ
- éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆaudioï¼‰ã®S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾å¿œ
- ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ï¼ˆæ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰
  ï¼ˆä¾‹: NHKG_TKY_20251015_0035-0125_AkxAQAELAAM_integrated_q1.00.jsonï¼‰

å¤‰æ›´å±¥æ­´:
v1.4: audioãƒ•ã‚©ãƒ«ãƒ€ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ã‚’è¿½åŠ 
      - processedé…ä¸‹ã®audioãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¢ç´¢ã—ã¦S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
      - ã™ã¹ã¦ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ï¼ˆq1.00ä»¥å¤–ã‚‚å«ã‚€ï¼‰
      - é¡ä¼¼ãƒ•ã‚¡ã‚¤ãƒ«åãŒã‚ã‚‹å ´åˆã€q1.00ã«è¿‘ã„æ–¹ã‚’å„ªå…ˆã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ 
      - ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã‚’æŠ½å‡ºã—ã¦æ¯”è¼ƒã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ 
v1.3: ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ã‚’è¿½åŠ 
      - S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å­˜åœ¨ç¢ºèªã¨æœ€çµ‚æ›´æ–°æ—¥æ™‚ã®æ¯”è¼ƒ
      - NASãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ—¥æ™‚ã¨S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æ›´æ–°æ—¥æ™‚ã‚’æ¯”è¼ƒ
      - æ›´æ–°ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿å†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆé‡è¤‡å›é¿ï¼‰
      - ã‚¹ã‚­ãƒƒãƒ—æ©Ÿèƒ½è¿½åŠ 
v1.2: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆscreenshotsï¼‰ã®S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ã‚’è¿½åŠ 
      - JSONå†…ã®Linuxãƒ‘ã‚¹ã‚’Windows NASãƒ‘ã‚¹ã«å¤‰æ›
      - screenshotsé…åˆ—ã‹ã‚‰ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã—ã¦S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
      - ç”»åƒURLã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«å«ã‚ã‚‹
v1.1: program-integrationé…ä¸‹å…¨ä½“ã‚’æ¢ç´¢ã€q1.00ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒãƒå‡¦ç†ã«å¯¾å¿œ
      - ãƒã‚±ãƒƒãƒˆåã‚’ã€Œtclip-raw-data-2025ã€ã«ä¿®æ­£
      - ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ã€Œap-northeast-1ã€ã«æ˜ç¤ºçš„ã«è¨­å®š

ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:
- boto3: S3æ“ä½œ
- jsonlines: JSON Lineså½¢å¼ã®å‡¦ç†
- sentence-transformers: ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
"""

import json
import jsonlines
import uuid
import boto3
import os
import re
import sys
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from datetime import datetime
from botocore.exceptions import ClientError

# ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("[WARNING] sentence-transformersãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    print("  ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install sentence-transformers")

# Windowsç’°å¢ƒã§ã®æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œ
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

# --- è¨­å®š ---
# S3è¨­å®š
S3_BUCKET_NAME = "tclip-raw-data-2025"
S3_REGION = "ap-northeast-1"  # ã‚¢ã‚¸ã‚¢ãƒ‘ã‚·ãƒ•ã‚£ãƒƒã‚¯ (æ±äº¬)
S3_MASTER_PREFIX = "rag/master_text/"
S3_CHUNK_PREFIX = "rag/vector_chunks/"
S3_IMAGE_PREFIX = "rag/images/"  # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
S3_AUDIO_PREFIX = "rag/audio/"  # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
S3_CLIENT = boto3.client('s3', region_name=S3_REGION)

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
BASE_NAS_PATH = r"\\NAS-TKY-2504\database\program-integration"
BASE_PROCESSED_NAS_PATH = r"\\NAS-TKY-2504\processed"

# --- ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·æŠ½å‡ºé–¢æ•° ---
def extract_version_number(filename: str) -> Optional[float]:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã‚’æŠ½å‡º
    ä¾‹: NHKG_TKY_20251015_0035-0125_AkxAQAELAAM_integrated_q1.00.json -> 1.00
    ä¾‹: NHKG_TKY_20251015_0035-0125_AkxAQAELAAM_integrated_q0.99.json -> 0.99
    æˆ»ã‚Šå€¤: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ï¼ˆfloatï¼‰ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯None
    """
    # ãƒ‘ã‚¿ãƒ¼ãƒ³: qæ•°å­—.æ•°å­—ï¼ˆä¾‹: q1.00, q0.99ï¼‰
    pattern = r'q(\d+\.\d+)'
    match = re.search(pattern, filename, re.IGNORECASE)
    if match:
        try:
            return float(match.group(1))
        except ValueError:
            return None
    return None

# --- ãƒ•ã‚¡ã‚¤ãƒ«åã®é¡ä¼¼åº¦åˆ¤å®šé–¢æ•° ---
def get_file_base_name(filename: str) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã‚’é™¤ã„ãŸãƒ™ãƒ¼ã‚¹åã‚’å–å¾—
    ä¾‹: NHKG_TKY_20251015_0035-0125_AkxAQAELAAM_integrated_q1.00.json
    -> NHKG_TKY_20251015_0035-0125_AkxAQAELAAM_integrated
    """
    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·éƒ¨åˆ†ã‚’å‰Šé™¤
    base_name = re.sub(r'_q\d+\.\d+', '', filename, flags=re.IGNORECASE)
    # .jsonæ‹¡å¼µå­ã‚’å‰Šé™¤
    base_name = re.sub(r'\.json$', '', base_name, flags=re.IGNORECASE)
    return base_name

# --- é¡ä¼¼ãƒ•ã‚¡ã‚¤ãƒ«ã®å„ªå…ˆé †ä½ä»˜ã‘ ---
def prioritize_files_by_version(file_paths: List[str]) -> List[str]:
    """
    é¡ä¼¼ãƒ•ã‚¡ã‚¤ãƒ«åãŒã‚ã‚‹å ´åˆã€q1.00ã«è¿‘ã„æ–¹ã‚’å„ªå…ˆã—ã¦ã‚½ãƒ¼ãƒˆ
    æˆ»ã‚Šå€¤: å„ªå…ˆé †ä½ãŒä»˜ã‘ã‚‰ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
    """
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹åã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    file_groups: Dict[str, List[Tuple[str, float]]] = {}
    
    for file_path in file_paths:
        filename = os.path.basename(file_path)
        base_name = get_file_base_name(filename)
        version = extract_version_number(filename)
        
        if base_name not in file_groups:
            file_groups[base_name] = []
        
        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ãŒãªã„å ´åˆã¯0.0ã¨ã—ã¦æ‰±ã†
        version_num = version if version is not None else 0.0
        file_groups[base_name].append((file_path, version_num))
    
    # å„ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§q1.00ã«è¿‘ã„é †ã«ã‚½ãƒ¼ãƒˆï¼ˆ1.00ã«è¿‘ã„æ–¹ãŒå„ªå…ˆï¼‰
    prioritized_files = []
    for base_name, files in file_groups.items():
        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ãŒ1.00ã«è¿‘ã„é †ã«ã‚½ãƒ¼ãƒˆ
        files.sort(key=lambda x: abs(1.00 - x[1]))
        prioritized_files.extend([f[0] for f in files])
    
    return prioritized_files

# --- ãƒ‘ã‚¹å¤‰æ›é–¢æ•° ---
def convert_linux_path_to_windows_nas(linux_path: str, channel_code: str = None, date_str: str = None) -> Optional[str]:
    r"""
    JSONå†…ã®Linuxãƒ‘ã‚¹å½¢å¼ã‚’Windows NASãƒ‘ã‚¹ã«å¤‰æ›ã™ã‚‹
    ä¾‹: /run/user/1000/gvfs/smb-share:server=nas-tky-2504.local,share=processed/NHKG-TKY/20251015AM/screenshot/xxx.jpeg
    -> \\NAS-TKY-2504\processed\NHKG-TKY\20251015AM\screenshot\xxx.jpeg
    
    ã¾ãŸã¯ screenshotsãƒ•ã‚©ãƒ«ãƒ€ã‚’è©¦è¡Œ
    """
    # Linuxãƒ‘ã‚¹ã‹ã‚‰ãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ—¥ä»˜ã€ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡º
    # ãƒ‘ã‚¿ãƒ¼ãƒ³: /run/user/.../share=processed/{CHANNEL}/{DATE}/screenshot(s)/{FILENAME}
    pattern = r'/share=processed/([^/]+)/([^/]+)/(?:screenshot|screenshots)/([^/]+\.jpeg)'
    match = re.search(pattern, linux_path)
    
    if not match:
        # ç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã‹ã‚‰æ§‹æˆã‚’è©¦è¡Œ
        filename = os.path.basename(linux_path)
        if channel_code and date_str:
            # ãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ—¥ä»˜ãŒæ—¢ã«åˆ†ã‹ã£ã¦ã„ã‚‹å ´åˆ
            for folder_name in ['screenshot', 'screenshots']:
                windows_path = os.path.join(BASE_PROCESSED_NAS_PATH, channel_code, date_str, folder_name, filename)
                if os.path.exists(windows_path):
                    return windows_path
        return None
    
    channel = match.group(1)
    date = match.group(2)
    filename = match.group(3)
    
    # screenshot ã¨ screenshots ã®ä¸¡æ–¹ã‚’è©¦è¡Œ
    for folder_name in ['screenshot', 'screenshots']:
        windows_path = os.path.join(BASE_PROCESSED_NAS_PATH, channel, date, folder_name, filename)
        if os.path.exists(windows_path):
            return windows_path
    
    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€åˆã®å€™è£œã‚’è¿”ã™ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ãŒã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¯å‘¼ã³å‡ºã—å´ã§ï¼‰
    return os.path.join(BASE_PROCESSED_NAS_PATH, channel, date, 'screenshot', filename)

# --- ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–¢æ•° ---
def upload_image_to_s3(image_path: str, doc_id: str, image_filename: str) -> Optional[str]:
    """
    ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    æˆ»ã‚Šå€¤: S3ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼ï¼ˆæˆåŠŸæ™‚ï¼‰ã€Noneï¼ˆå¤±æ•—æ™‚ï¼‰
    """
    try:
        if not os.path.exists(image_path):
            print(f"[WARNING] ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_path}")
            return None
        
        # S3ã®ã‚­ãƒ¼ã‚’ç”Ÿæˆ: rag/images/{doc_id}/{filename}
        # ãƒ•ã‚¡ã‚¤ãƒ«åã®è¡çªã‚’é¿ã‘ã‚‹ãŸã‚ã€doc_idã§ãƒ•ã‚©ãƒ«ãƒ€åˆ†ã‘
        s3_key = f"{S3_IMAGE_PREFIX}{doc_id}/{image_filename}"
        
        # Content-Typeã‚’è¨­å®š
        content_type = 'image/jpeg'
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        with open(image_path, 'rb') as f:
            S3_CLIENT.put_object(
                Bucket=S3_BUCKET_NAME,
                Key=s3_key,
                Body=f.read(),
                ContentType=content_type
            )
        
        s3_url = f"s3://{S3_BUCKET_NAME}/{s3_key}"
        return s3_url
        
    except Exception as e:
        print(f"[ERROR] ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {image_path} - {str(e)}")
        return None

# --- éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–¢æ•° ---
def upload_audio_to_s3(audio_path: str, doc_id: str, audio_filename: str) -> Optional[str]:
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    æˆ»ã‚Šå€¤: S3ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼ï¼ˆæˆåŠŸæ™‚ï¼‰ã€Noneï¼ˆå¤±æ•—æ™‚ï¼‰
    """
    try:
        if not os.path.exists(audio_path):
            print(f"[WARNING] éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {audio_path}")
            return None
        
        # S3ã®ã‚­ãƒ¼ã‚’ç”Ÿæˆ: rag/audio/{doc_id}/{filename}
        # ãƒ•ã‚¡ã‚¤ãƒ«åã®è¡çªã‚’é¿ã‘ã‚‹ãŸã‚ã€doc_idã§ãƒ•ã‚©ãƒ«ãƒ€åˆ†ã‘
        s3_key = f"{S3_AUDIO_PREFIX}{doc_id}/{audio_filename}"
        
        # Content-Typeã‚’è¨­å®šï¼ˆæ‹¡å¼µå­ã‹ã‚‰åˆ¤å®šï¼‰
        ext = os.path.splitext(audio_filename)[1].lower()
        content_type_map = {
            '.mp3': 'audio/mpeg',
            '.wav': 'audio/wav',
            '.m4a': 'audio/mp4',
            '.aac': 'audio/aac',
            '.ogg': 'audio/ogg',
            '.flac': 'audio/flac'
        }
        content_type = content_type_map.get(ext, 'audio/mpeg')
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        with open(audio_path, 'rb') as f:
            S3_CLIENT.put_object(
                Bucket=S3_BUCKET_NAME,
                Key=s3_key,
                Body=f.read(),
                ContentType=content_type
            )
        
        s3_url = f"s3://{S3_BUCKET_NAME}/{s3_key}"
        return s3_url
        
    except Exception as e:
        print(f"[ERROR] éŸ³å£°ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {audio_path} - {str(e)}")
        return None

# --- éŸ³å£°ãƒ•ã‚©ãƒ«ãƒ€å‡¦ç†é–¢æ•° ---
def process_and_upload_audio_files(doc_id: str, channel_code: str, date_str: str) -> List[str]:
    """
    processedé…ä¸‹ã®audioãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢ã—ã¦S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    æˆ»ã‚Šå€¤: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®S3 URLãƒªã‚¹ãƒˆ
    """
    uploaded_audio_urls = []
    
    # audioãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ã‚’æ§‹ç¯‰ï¼ˆè¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œï¼‰
    # ä¾‹: \\NAS-TKY-2504\processed\NHKG-TKY\20251003AM\audio
    # ã¾ãŸã¯: \\NAS-TKY-2504\processed\NHKG-TKY\20251003\audio
    audio_base_paths = []
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ—¥ä»˜ã®ã¿ï¼ˆä¾‹: 20251003ï¼‰
    audio_base_paths.append(os.path.join(BASE_PROCESSED_NAS_PATH, channel_code, date_str, 'audio'))
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ—¥ä»˜+AMï¼ˆä¾‹: 20251003AMï¼‰
    audio_base_paths.append(os.path.join(BASE_PROCESSED_NAS_PATH, channel_code, f"{date_str}AM", 'audio'))
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: æ—¥ä»˜+PMï¼ˆä¾‹: 20251003PMï¼‰
    audio_base_paths.append(os.path.join(BASE_PROCESSED_NAS_PATH, channel_code, f"{date_str}PM", 'audio'))
    
    # å­˜åœ¨ã™ã‚‹ãƒ‘ã‚¹ã‚’æ¢ã™
    audio_base_path = None
    for path in audio_base_paths:
        if os.path.exists(path):
            audio_base_path = path
            break
    
    if not audio_base_path:
        print(f"[INFO] audioãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆè©¦è¡Œã—ãŸãƒ‘ã‚¹: {audio_base_paths}ï¼‰")
        return uploaded_audio_urls
    
    print(f"[INFO] audioãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¢ç´¢ä¸­: {audio_base_path}")
    
    # audioãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
    try:
        for root, dirs, files in os.walk(audio_base_path):
            for filename in files:
                # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã‚’ãƒã‚§ãƒƒã‚¯
                ext = os.path.splitext(filename)[1].lower()
                if ext in ['.mp3', '.wav', '.m4a', '.aac', '.ogg', '.flac']:
                    audio_path = os.path.join(root, filename)
                    s3_url = upload_audio_to_s3(audio_path, doc_id, filename)
                    if s3_url:
                        uploaded_audio_urls.append(s3_url)
                        print(f"[OK] éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {s3_url}")
                    else:
                        print(f"[WARNING] éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {audio_path}")
    except Exception as e:
        print(f"[ERROR] audioãƒ•ã‚©ãƒ«ãƒ€æ¢ç´¢ã‚¨ãƒ©ãƒ¼: {audio_base_path} - {str(e)}")
    
    return uploaded_audio_urls

# --- åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰ ---
_embedding_model = None

def get_embedding_model():
    """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰"""
    global _embedding_model
    if _embedding_model is None and SENTENCE_TRANSFORMERS_AVAILABLE:
        try:
            print("[INFO] åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            _embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            print("[OK] åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†")
        except Exception as e:
            print(f"[ERROR] åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    return _embedding_model

# --- ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥ (ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã€ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—è¿½åŠ ) ---
def segment_based_chunking(transcripts: List[Dict], doc_id: str) -> List[Dict[str, Any]]:
    """
    ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’åŸºæœ¬ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã€æ™‚é–“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä»˜ä¸ã™ã‚‹ã€‚
    ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ã‚‚å®Ÿè¡Œã—ã¦ãƒãƒ£ãƒ³ã‚¯ã«è¿½åŠ ã™ã‚‹ã€‚
    """
    chunks = []
    
    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
    model = get_embedding_model()
    
    # ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆé…åˆ—ã®å„è¦ç´ ã‚’ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦åˆ©ç”¨
    for i, segment in enumerate(transcripts):
        if 'content' not in segment or 'file_path' not in segment:
             continue 
        
        # 'content'ä»¥å¤–ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯ã€å…ƒã®çµ±åˆJSONã‹ã‚‰ã‚³ãƒ”ãƒ¼
        # ğŸ’¡ start_time_msã¨end_time_msã¯ã€çµ±åˆJSONã®'transcripts'è¦ç´ ã‹ã‚‰ç›´æ¥æŠ½å‡ºã•ã‚Œã‚‹ã¨ä»®å®šï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªã®å•é¡Œã«ã‚ˆã‚Šã€ä»Šå›ã¯'content'ãŒã‚ã‚‹ã‹ã®ã¿ãƒã‚§ãƒƒã‚¯ï¼‰
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®æ–‡å­—åˆ—ã‹ã‚‰é–‹å§‹ãƒ»çµ‚äº†æ™‚é–“ã®æ–‡å­—åˆ—ã‚’æŠ½å‡ºï¼ˆã‚ˆã‚Šæ­£ç¢ºãªæ™‚é–“æƒ…å ±ãŒã‚ã‚Œã°ãã¡ã‚‰ã‚’ä½¿ç”¨ã™ã¹ãï¼‰
        time_match = re.search(r'(\d{8}-\d{6})-\d+-', segment.get('file_name', segment.get('file_path', '')))
        
        chunk_id = f"{doc_id}-p{i:04d}" # ä¸€æ„ãªãƒãƒ£ãƒ³ã‚¯IDï¼ˆdoc_id + ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ™‚é–“æƒ…å ±ã‚„å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ ¼ç´
        metadata = {
            "source": "transcript",
            # JSONã«æ™‚é–“æƒ…å ±ãŒç„¡ã„å ´åˆã€ã“ã“ã§ã¯ä¸€æ—¦ç©ºã«ã™ã‚‹ã‹ã€ã‚ˆã‚Šå …ç‰¢ãªæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦
            "start_time": segment.get('start_time'), # çµ±åˆJSONã®æ§‹é€ ã«ä¾å­˜
            "end_time": segment.get('end_time'),     # çµ±åˆJSONã®æ§‹é€ ã«ä¾å­˜
            "original_file_path": segment['file_path'] 
        }

        chunk_text = segment['content']
        chunk = {
            "chunk_id": chunk_id,
            "doc_id": doc_id,
            "text": chunk_text, # ãƒãƒ£ãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆ
            "level": "segment",
            "metadata": metadata
        }
        
        # ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if model is not None and chunk_text:
            try:
                embedding = model.encode(chunk_text, convert_to_numpy=True)
                # ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒªã‚¹ãƒˆå½¢å¼ã§ä¿å­˜ï¼ˆJSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ã«ã™ã‚‹ãŸã‚ï¼‰
                chunk['embedding'] = embedding.tolist()
            except Exception as e:
                print(f"[WARNING] ãƒãƒ£ãƒ³ã‚¯ {chunk_id} ã®ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒãƒ£ãƒ³ã‚¯ã¯è¿½åŠ ã™ã‚‹ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ãªã—ï¼‰
        
        chunks.append(chunk)
        
    return chunks

# --- S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå­˜åœ¨ç¢ºèªã¨æ›´æ–°æ—¥æ™‚å–å¾— ---
def get_s3_object_metadata(key: str) -> Optional[Dict]:
    """
    S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
    æˆ»ã‚Šå€¤: {'exists': True, 'LastModified': datetime} ã¾ãŸã¯ None
    """
    try:
        response = S3_CLIENT.head_object(Bucket=S3_BUCKET_NAME, Key=key)
        return {
            'exists': True,
            'LastModified': response['LastModified'],
            'ETag': response.get('ETag', ''),
            'Size': response.get('ContentLength', 0)
        }
    except ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == '404':
            return {'exists': False}
        else:
            print(f"[WARNING] S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç¢ºèªã‚¨ãƒ©ãƒ¼: {key} - {error_code}")
            return None

# --- ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ãƒã‚§ãƒƒã‚¯ ---
def should_upload_file(file_path: str, s3_key: str) -> Tuple[bool, str]:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã¹ãã‹ãƒã‚§ãƒƒã‚¯
    æˆ»ã‚Šå€¤: (ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã¹ãã‹, ç†ç”±)
    """
    try:
        # NASãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ—¥æ™‚ã‚’å–å¾—
        file_mtime = os.path.getmtime(file_path)
        file_datetime = datetime.fromtimestamp(file_mtime)
        
        # S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        s3_metadata = get_s3_object_metadata(s3_key)
        
        if not s3_metadata or not s3_metadata.get('exists'):
            return (True, "S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå­˜åœ¨ã—ãªã„")
        
        # S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æœ€çµ‚æ›´æ–°æ—¥æ™‚ï¼ˆUTCï¼‰
        s3_last_modified = s3_metadata['LastModified']
        
        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’è€ƒæ…®ã—ã¦æ¯”è¼ƒ
        # S3ã¯UTCã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã¯ãƒ­ãƒ¼ã‚«ãƒ«æ™‚åˆ»ãªã®ã§ã€timezone-naiveã§æ¯”è¼ƒ
        # ä¸¡æ–¹ã‚’naive datetimeã«å¤‰æ›ã—ã¦æ¯”è¼ƒ
        s3_datetime_naive = s3_last_modified.replace(tzinfo=None) if s3_last_modified.tzinfo else s3_last_modified
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ—¥æ™‚ãŒS3ã‚ˆã‚Šæ–°ã—ã„å ´åˆã®ã¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        # 5ç§’ã®ãƒãƒ¼ã‚¸ãƒ³ã‚’è¨­å®šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³èª¤å·®ã‚’è€ƒæ…®ï¼‰
        time_diff = (file_datetime - s3_datetime_naive).total_seconds()
        if time_diff > 5:
            return (True, f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ›´æ–°ã•ã‚Œã¦ã„ã¾ã™ (NAS: {file_datetime}, S3: {s3_datetime_naive}, å·®åˆ†: {time_diff:.1f}ç§’)")
        else:
            return (False, f"ãƒ•ã‚¡ã‚¤ãƒ«ã¯æœ€æ–°ã§ã™ (ã‚¹ã‚­ãƒƒãƒ—)")
    
    except Exception as e:
        print(f"[WARNING] æ›´æ–°ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {file_path} - {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨ã®ãŸã‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        return (True, f"ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šæ›´æ–°ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—: {str(e)}")

# --- S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç† ---
def upload_to_s3(data_list: List[Dict], key: str, skip_if_exists: bool = False):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’JSON Lineså½¢å¼ã§S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    skip_if_exists: Trueã®å ´åˆã€æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ›´æ–°ãƒã‚§ãƒƒã‚¯ã¯å‘¼ã³å‡ºã—å´ã§å®Ÿæ–½ï¼‰
    """
    data_str = ""
    for item in data_list:
        data_str += json.dumps(item, ensure_ascii=False) + "\n"
        
    # S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ“ä½œ
    S3_CLIENT.put_object(
        Bucket=S3_BUCKET_NAME,
        Key=key,
        Body=data_str.encode('utf-8'),
        ContentType='application/jsonl; charset=utf-8'
    )
    print(f"[OK] S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: s3://{S3_BUCKET_NAME}/{key}")


# --- ãƒ•ã‚¡ã‚¤ãƒ«æ¢ç´¢é–¢æ•°ï¼ˆã™ã¹ã¦ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å‡¦ç†ï¼‰ ---
def find_all_json_files(root_path: str) -> List[str]:
    """
    program-integrationé…ä¸‹ã‚’å†å¸°çš„ã«æ¢ç´¢ã—ã€ã™ã¹ã¦ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¿”ã™
    ï¼ˆä¾‹: NHKG_TKY_20251015_0035-0125_AkxAQAELAAM_integrated_q1.00.jsonï¼‰
    ï¼ˆä¾‹: NHKG_TKY_20251015_0035-0125_AkxAQAELAAM_integrated_q0.99.jsonï¼‰
    """
    json_files = []
    
    if not os.path.exists(root_path):
        print(f"[ERROR] ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {root_path}")
        return json_files
    
    print(f"[INFO] ãƒ•ã‚¡ã‚¤ãƒ«æ¢ç´¢ã‚’é–‹å§‹: {root_path}")
    
    # å†å¸°çš„ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
    for root, dirs, files in os.walk(root_path):
        for filename in files:
            # .json ã§çµ‚ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã™ã¹ã¦å–å¾—ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã¯å•ã‚ãªã„ï¼‰
            if filename.lower().endswith('.json'):
                # integratedãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å¯¾è±¡ï¼ˆçµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
                if 'integrated' in filename.lower():
                    file_path = os.path.join(root, filename)
                    json_files.append(file_path)
                    version = extract_version_number(filename)
                    version_str = f"q{version:.2f}" if version else "ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸æ˜"
                    print(f"[INFO] JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹: {file_path} (ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {version_str})")
    
    # é¡ä¼¼ãƒ•ã‚¡ã‚¤ãƒ«åãŒã‚ã‚‹å ´åˆã€q1.00ã«è¿‘ã„æ–¹ã‚’å„ªå…ˆ
    prioritized_files = prioritize_files_by_version(json_files)
    
    return prioritized_files

# --- ç”»åƒå‡¦ç†é–¢æ•° ---
def process_and_upload_images(integrated_data: Dict, doc_id: str) -> List[str]:
    """
    JSONå†…ã®screenshotsé…åˆ—ã‹ã‚‰ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    æˆ»ã‚Šå€¤: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã®S3 URLãƒªã‚¹ãƒˆ
    """
    uploaded_image_urls = []
    
    if 'screenshots' not in integrated_data or not integrated_data['screenshots']:
        print(f"[INFO] ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆdoc_id: {doc_id}ï¼‰")
        return uploaded_image_urls
    
    # ãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ—¥ä»˜ã‚’å–å¾—ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚„ãƒ‘ã‚¹ã‹ã‚‰æŠ½å‡ºã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯Noneã®ã¾ã¾ï¼ˆãƒ‘ã‚¹å¤‰æ›é–¢æ•°å†…ã§å‡¦ç†ï¼‰
    channel_code = None
    date_str = None
    
    print(f"[INFO] {len(integrated_data['screenshots'])}å€‹ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­...")
    
    for screenshot in integrated_data['screenshots']:
        linux_path = screenshot.get('file_path', '')
        filename = screenshot.get('file_name', '')
        
        if not linux_path or not filename:
            continue
        
        # ãƒ‘ã‚¹å¤‰æ›
        windows_path = convert_linux_path_to_windows_nas(linux_path, channel_code, date_str)
        
        if not windows_path:
            # ãƒ‘ã‚¹å¤‰æ›ã«å¤±æ•—ã—ãŸå ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç›´æ¥æ§‹ç¯‰ã‚’è©¦è¡Œ
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã¨ãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            # ä¾‹: NHKG-TKY-20251015-003534-xxx.jpeg
            filename_match = re.search(r'([A-Z]+-[A-Z]+)-(\d{8})', filename)
            if filename_match:
                channel = filename_match.group(1)
                date = filename_match.group(2)
                # æ—¥ä»˜å½¢å¼ã‚’å¤‰æ›: 20251015 -> 20251015AM ã¾ãŸã¯ä»–ã®å½¢å¼
                # ã“ã“ã§ã¯ä¸€æ—¦ãã®ã¾ã¾ä½¿ç”¨
                for folder_name in ['screenshot', 'screenshots']:
                    test_path = os.path.join(BASE_PROCESSED_NAS_PATH, channel, date, folder_name, filename)
                    if os.path.exists(test_path):
                        windows_path = test_path
                        break
                    # AM/PMãªã©ã®ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è©¦è¡Œ
                    for suffix in ['AM', 'PM']:
                        test_path = os.path.join(BASE_PROCESSED_NAS_PATH, channel, f"{date}{suffix}", folder_name, filename)
                        if os.path.exists(test_path):
                            windows_path = test_path
                            break
        
        if windows_path and os.path.exists(windows_path):
            s3_url = upload_image_to_s3(windows_path, doc_id, filename)
            if s3_url:
                uploaded_image_urls.append(s3_url)
                print(f"[OK] ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {s3_url}")
            else:
                print(f"[WARNING] ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {windows_path}")
        else:
            print(f"[WARNING] ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {linux_path} (å¤‰æ›å¾Œ: {windows_path})")
    
    return uploaded_image_urls

# --- å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–¢æ•° ---
def process_single_file(file_path: str) -> bool:
    """
    å˜ä¸€ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    æˆåŠŸã—ãŸå ´åˆã¯Trueã€å¤±æ•—ã—ãŸå ´åˆã¯Falseã‚’è¿”ã™
    """
    try:
        print(f"\n[INFO] ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚’é–‹å§‹: {file_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        if not os.path.exists(file_path):
            print(f"[WARNING] ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {file_path}")
            return False
        
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        with open(file_path, 'r', encoding='utf-8') as f:
            integrated_data = json.load(f)
        
        # event_idã®å–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¿½åŠ ï¼‰
        if 'program_metadata' not in integrated_data or 'event_id' not in integrated_data['program_metadata']:
            print(f"[ERROR] program_metadata.event_id ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return False
        
        doc_id = integrated_data['program_metadata']['event_id']
        print(f"[INFO] doc_id: {doc_id}")
        
        # ãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ—¥ä»˜ã‚’å–å¾—ï¼ˆaudioãƒ•ã‚©ãƒ«ãƒ€æ¢ç´¢ç”¨ï¼‰
        channel_code = None
        date_str = None
        if 'program_metadata' in integrated_data:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ—¥ä»˜ã‚’å–å¾—
            metadata = integrated_data['program_metadata']
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚‚æŠ½å‡ºã‚’è©¦è¡Œ
            filename = os.path.basename(file_path)
            # ä¾‹: NHKG_TKY_20251015_0035-0125_AkxAQAELAAM_integrated_q1.00.json
            match = re.search(r'([A-Z]+_[A-Z]+)_(\d{8})', filename)
            if match:
                channel_code = match.group(1).replace('_', '-')  # NHKG_TKY -> NHKG-TKY
                date_str = match.group(2)  # 20251015
        
        # transcriptsã®å­˜åœ¨ç¢ºèª
        if 'transcripts' not in integrated_data:
            print(f"[ERROR] transcripts ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return False
        
        # 1. ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        master_data = {
            "doc_id": doc_id,
            "metadata": integrated_data['program_metadata'],
            "full_text": "".join([t['content'] for t in integrated_data['transcripts'] if 'content' in t])
        }
        
        # 2. ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        all_chunks = segment_based_chunking(integrated_data['transcripts'], doc_id)
        
        if not all_chunks:
            print(f"[WARNING] ãƒãƒ£ãƒ³ã‚¯ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ: {file_path}")
            return False
        
        print(f"[INFO] {len(all_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆ")
        
        # --- æ›´æ–°ãƒã‚§ãƒƒã‚¯ ---
        master_key = f"{S3_MASTER_PREFIX}{doc_id}.jsonl"
        chunk_key = f"{S3_CHUNK_PREFIX}{doc_id}_segments.jsonl"
        
        # ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ãƒã‚§ãƒƒã‚¯
        should_upload_master, reason_master = should_upload_file(file_path, master_key)
        should_upload_chunk, reason_chunk = should_upload_file(file_path, chunk_key)
        
        # ã©ã¡ã‚‰ã‹ãŒæ›´æ–°ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not should_upload_master and not should_upload_chunk:
            print(f"[SKIP] ãƒ•ã‚¡ã‚¤ãƒ«ã¯æœ€æ–°ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {file_path}")
            print(f"  ç†ç”±: {reason_master}")
            return True
        
        if should_upload_master:
            print(f"[INFO] ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°: {reason_master}")
        if should_upload_chunk:
            print(f"[INFO] ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°: {reason_chunk}")
        
        # 3. ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        uploaded_image_urls = process_and_upload_images(integrated_data, doc_id)
        
        # ç”»åƒURLã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
        if uploaded_image_urls:
            master_data['image_urls'] = uploaded_image_urls
            print(f"[INFO] {len(uploaded_image_urls)}å€‹ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
        
        # 4. éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        if channel_code and date_str:
            uploaded_audio_urls = process_and_upload_audio_files(doc_id, channel_code, date_str)
            if uploaded_audio_urls:
                master_data['audio_urls'] = uploaded_audio_urls
                print(f"[INFO] {len(uploaded_audio_urls)}å€‹ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
        else:
            print(f"[WARNING] ãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ã¾ãŸã¯æ—¥ä»˜ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆaudioãƒ•ã‚©ãƒ«ãƒ€æ¢ç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        
        # --- S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ ---
        
        # A. ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ (PostgreSQLã®å…¥åŠ›ç”¨)
        if should_upload_master:
            upload_to_s3([master_data], master_key)
        else:
            print(f"[SKIP] ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {master_key}")
        
        # B. ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ (Weaviate/OpenSearchã®å…¥åŠ›ç”¨)
        if should_upload_chunk:
            upload_to_s3(all_chunks, chunk_key)
        else:
            print(f"[SKIP] ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {chunk_key}")
        
        print(f"[OK] ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {file_path}")
        return True
        
    except json.JSONDecodeError as e:
        print(f"[ERROR] JSONè§£æã‚¨ãƒ©ãƒ¼: {file_path} - {str(e)}")
        return False
    except Exception as e:
        print(f"[ERROR] å‡¦ç†ã‚¨ãƒ©ãƒ¼: {file_path} - {str(e)}")
        import traceback
        traceback.print_exc()
        return False

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† (ãƒãƒƒãƒå‡¦ç†å¯¾å¿œ) ---
def process_and_upload_local_rag_data():
    """
    program-integrationé…ä¸‹ã®ã™ã¹ã¦ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢ã—ã€ãƒãƒƒãƒå‡¦ç†ã§S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    é¡ä¼¼ãƒ•ã‚¡ã‚¤ãƒ«åãŒã‚ã‚‹å ´åˆã€q1.00ã«è¿‘ã„æ–¹ã‚’å„ªå…ˆ
    """
    # ã™ã¹ã¦ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
    json_files = find_all_json_files(BASE_NAS_PATH)
    
    if not json_files:
        print(f"[WARNING] JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    print(f"\n[INFO] åˆè¨ˆ {len(json_files)} å€‹ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
    print("=" * 80)
    
    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    success_count = 0
    error_count = 0
    
    for i, file_path in enumerate(json_files, 1):
        print(f"\n[{i}/{len(json_files)}] å‡¦ç†ä¸­...")
        if process_single_file(file_path):
            success_count += 1
        else:
            error_count += 1
    
    # å‡¦ç†çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 80)
    print(f"[SUMMARY] å‡¦ç†å®Œäº†")
    print(f"  æˆåŠŸ: {success_count} ãƒ•ã‚¡ã‚¤ãƒ«")
    print(f"  å¤±æ•—: {error_count} ãƒ•ã‚¡ã‚¤ãƒ«")
    print(f"  åˆè¨ˆ: {len(json_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
    
# --- å®Ÿè¡Œä¾‹ ---
# å®Ÿè¡Œã™ã‚‹å‰ã«ã€BASE_NAS_PATHãŒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµŒç”±ã§Pythonã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

if __name__ == "__main__":
    # ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œï¼ˆã™ã¹ã¦ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒãƒå‡¦ç† + ç”»åƒãƒ»éŸ³å£°ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰
    process_and_upload_local_rag_data()

